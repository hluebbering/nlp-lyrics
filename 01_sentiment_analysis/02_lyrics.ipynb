{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Using the data gathered from the Spotify API, we now want to extract and process the lyrics for each song. This is accomplished through scraping textual information, namely lyrical data, from the **Genius Lyrics** website. Following extraction, the lyrics are thoroughly processed and cleaned before undergoing sentiment analysis. \n",
    "\n",
    "\n",
    "\n",
    "<!--### Scraping the Genius Lyrics Website\n",
    "scraping textual information\n",
    "Scraping the Genius Lyrics Website-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-code",
     "hide-warning",
     "hide-message"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "import string\n",
    "from better_profanity import profanity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import lyricsgenius\n",
    "from transformers import pipeline\n",
    "from spellchecker import SpellChecker\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "all_tracks = pd.read_csv(\"../assets/data/all_tracks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Scraping the Web\n",
    "\n",
    "To get started, the script below imports `lyricsgenius`, a fundamental package libary allowing for web scraping of the Genius Lyrics website to retrieve the lyrics of any given song. Through the initialization of the `genius` variable, one can access the Genius API and retrieve the lyrics of any given song, such as \"Too Many Nights\" by Metro Boomin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Too Many Nights\" by Metro Boomin...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import lyricsgenius\n",
    "\n",
    "genius = lyricsgenius.Genius(\"epFCxujgBe-Y6WrkZedI8kerKxiCpR6Rh0DAHYNlKDf9B4H1nXTdZIkj7krNUHVV\")\n",
    "song = genius.search_song(\"Too Many Nights\", \"Metro Boomin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function that retrieves the lyrics for any song and artist from the Genius database. As shown below, it first searches for the track using the provided name and artist and then extracts the lyrics from the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_lyrics(song_name, song_artist):\n",
    "    song_genius = genius.search_song(song_name, song_artist)\n",
    "    song_lyrics = song_genius.lyrics.partition(\"Lyrics\")[2]\n",
    "    # Remove any numbers followed by 'Embed'\n",
    "    song_lyrics = re.sub(r\"[\\[].*?[\\]]|\\d+Embed\", \"\", song_lyrics)\n",
    "    # Remove text between square brackets\n",
    "    song_lyrics = re.sub(r\"(\\-[A-Za-z]+\\-)\", \"\", song_lyrics)\n",
    "\n",
    "    return song_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text Data\n",
    "\n",
    "The following Python script contains various functions optimized for efficiently cleaning song lyrics, which is a crucial step towards building a sentiment classifier. The text pre-processing procedure involves the following main steps.\n",
    "\n",
    "1. Language Detection\n",
    "2. Expanding Contractions\n",
    "3. Converting Text to Lowercase\n",
    "4. Spell Checking + Censoring\n",
    "5. Removing Punctuations\n",
    "6. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `detect_and_translate` below is designed to identify and translate text into a specified language, specifically English. It first checks the language of the original text and compares it to the target language. If the detected language differs from the target language, the function utilizes GoogleTranslator to translate the input text into the target language (English).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and translate text\n",
    "def detect_and_translate(track_lyrics, target_lang='en'):\n",
    "    if detect(track_lyrics) == target_lang:\n",
    "        return track_lyrics\n",
    "    translator = GoogleTranslator(source='auto', target=target_lang)\n",
    "    return translator.translate(track_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also develop various functions to support the preprocessing of textual data, streamlining the process and improving the accuracy of the final output. Among these functions are a method for removing punctuation from a given string of lyrics and a spell-checker that automatically finds and corrects any spelling errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    no_punct = \"\"\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct  # return unpunctuated string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell Check + Censor\n",
    "spell = SpellChecker()\n",
    "\n",
    "def spell_check(word_list_str):\n",
    "    word_corrected_list = []\n",
    "    for word in word_list_str.split():\n",
    "        word_corrected = spell.correction(word)\n",
    "        if word_corrected is not None:\n",
    "            word_corrected_list.append(word_corrected)\n",
    "        else:\n",
    "            word_corrected_list.append(word)\n",
    "    return word_corrected_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_song_lyrics` function is designed to simplify the processing of lyrics for a specific song and artist. The function extracts the lyrics from the Genius database and performs a series of modifications, including expanding contractions, removing repetitive phrases, and converting the text to lowercase. It also ensures that the spelling is correct and eliminates any profanity. The end result is a cleaned set of lyrics, tokenized and encoded as a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_song_lyrics(song_name, artist_name):\n",
    "    genius_lyrics = get_song_lyrics(song_name, artist_name) # <1>\n",
    "    lyrics_en = detect_and_translate(genius_lyrics, \"en\")  # <2> \n",
    "    \n",
    "    no_contract = [contractions.fix(word) for word in lyrics_en.split()] # <3>\n",
    "    no_contract_str = \" \".join(no_contract).lower()  # lowercase # <4>\n",
    "    no_contract_str = re.sub(r\"nana|lala\", \"\", no_contract_str) # <4>\n",
    "    \n",
    "    corrected = spell_check(no_contract_str) # <5> # Spell Check + Censor\n",
    "    censored = profanity.censor(\" \".join(corrected), censor_char=\"\") # <5>\n",
    "    no_punct = remove_punctuation(censored) # <6> # Remove Punctuation\n",
    "    \n",
    "    tokenized = word_tokenize(no_punct)  # Tokenize # <7>\n",
    "    strencode = [i.encode(\"ascii\", \"ignore\") for i in tokenized]  # Encode() method # <8>\n",
    "    return [i.decode() for i in strencode]  # Decode() method # <8>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words\n",
    "\n",
    "The code below aims to eliminate stopwords from lyrics utilizing the **Natural Language Toolkit** (NLTK) library and its `WordNetLemmatizer` tool. By eliminating frequently occurring words like \"the,\" \"and,\" or \"of,\" the resulting text becomes more compact and meaningful. Without the distraction of stopwords, the analysis can more effectively capture the essence of the lyrics and the underlying conveyed message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_lyrics(clean_lyrics_decode):\n",
    "    stopword = stopwords.words(\"english\")\n",
    "    stopword.extend([\"yeah\", \"nanana\", \"nana\", \"oh\", \"la\"])\n",
    "    return [word for word in clean_lyrics_decode if word not in stopword]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "\n",
    "\n",
    "Next, we define a function to perform lemmatization on a set of words using the `WordNetLemmatizer` class from the NLTK library. Lemmatization helps to standardize words and reduce their complexity by reducing words to their root or base form. Our function specifically targets verbs and transforms different variations of the same verb into its most basic form.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk import pos_tag\n",
    "\n",
    "def word_lemmatize(lyrics_cleaned):  # clean_lyrics_decode):\n",
    "    pos_tags = pos_tag(lyrics_cleaned)\n",
    "    wordnet_pos = [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos_tags]\n",
    "\n",
    "    wnl = WordNetLemmatizer()  # Lemmatize Lyrics\n",
    "    return [wnl.lemmatize(word, tag) for word, tag in wordnet_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the code above defines a function that makes use of the WordNetLemmatizer class from the NLTK library to conduct lemmatization specifically targeting verbs, thereby converting words to their most basic form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "\n",
    "Subsequently, the process involves the implementation of pipeline classes to carry out predictions using models accessible in the Hub. The code imports and employs multiple transformer models specifically designed for text classification and sentiment analysis. Specifically, the following procedure creates three distinct pipelines, each equipped with different models that facilitate the assessment of emotions and sentiment in textual content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-code"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# python -m pip install \"tensorflow<2.11\"\n",
    "# python -m pip install \"protobuf<3.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize Genius API and sentiment classifiers\n",
    "classifiers = [\n",
    "    pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True),\n",
    "    pipeline(\"text-classification\", model='cardiffnlp/twitter-roberta-base-sentiment', return_all_scores=True),\n",
    "    pipeline(\"sentiment-analysis\", return_all_scores=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_lyric_sentiment` function takes in pre-processed lyrics as input and produces a dictionary of sentiment scores. It leverages three distinct classifiers to calculate the scores and aggregates them into a final result. For instance, one of these classifiers is the *distilbert-base-uncased-emotion* model, specifically trained to detect \"emotions in texts such as sadness, joy, love, anger, fear, and surprise\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis\n",
    "def get_lyric_sentiment(lyrics, classifiers):\n",
    "    text = ' '.join(lyrics)\n",
    "    scores = {}\n",
    "    for classifier in classifiers:\n",
    "        try:\n",
    "            predictions = classifier(text, truncation=True)\n",
    "            for prediction in predictions[0]:\n",
    "                scores[prediction['label']] = prediction['score']\n",
    "        except Exception as e:\n",
    "            print(f\"Error during sentiment analysis: {e}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the lyric sequence contains more than 512 tokens, it will trigger an error message indicating an exception encountered in the 'embeddings' layer. However, we have implemented measures to properly manage lyric sequences that exceed 512 words in the function mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "\n",
    "## Putting it All Together\n",
    "\n",
    "\n",
    "To summarize, the code efficiently collects data and performs text analysis on every song in a playlist. Specifically, it systematically processes a list of tracks and corresponding artists while simultaneously conducting a thorough cleaning procedure on the lyrics. The cleaning process involves removing all nonessential characters, resulting in a more precise depiction of the song's content. The outcome is a comprehensive frequency analysis of each word in a song's lyrics, providing deeper insights into the overall conveyed message.\n",
    "\n",
    "Additionally, the program computes a sentiment score for each song based on the lyrics, indicating whether the lyrics are positive, negative, or neutral. It also collects information about the song and artist, such as the release date, length, popularity, and genre. Finally, the program compiles all this information into a dataframe for further analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Beautiful Things\" by Benson Boone...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"obsessed\" by Olivia Rodrigo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"we can't be friends (wait for your love)\" by Ariana Grande...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Lose Control\" by Teddy Swims...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"greedy\" by Tate McRae...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"TEXAS HOLD 'EM\" by Beyoncé...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"End of Beginning\" by Djo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Stick Season\" by Noah Kahan...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Saturn\" by SZA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"redrum\" by 21 Savage...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Training Season\" by Dua Lipa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Water\" by Tyla...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Feather\" by Sabrina Carpenter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Lovin On Me\" by Jack Harlow...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"One Of The Girls\" by The Weeknd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Paint The Town Red\" by Doja Cat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Cruel Summer\" by Taylor Swift...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Scared To Start\" by Michael Marcagi...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"yes, and?\" by Ariana Grande...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Strangers\" by Kenya Grace...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"My Love Mine All Mine\" by Mitski...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"I Remember Everything (feat. Kacey Musgraves)\" by Zach Bryan...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sentiment analysis: The expanded size of the tensor (528) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 528].  Tensor sizes: [1, 514]\n",
      "Searching for \"exes\" by Tate McRae...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Puntería\" by Shakira...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Whatever She Wants\" by Bryson Tiller...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Houdini\" by Dua Lipa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Snooze\" by SZA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Agora Hills\" by Doja Cat...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Type Shit\" by Future...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Never Lose Me (feat. SZA & Cardi B)\" by Flo Milli...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sentiment analysis: The expanded size of the tensor (525) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 525].  Tensor sizes: [1, 514]\n",
      "Searching for \"Made For Me\" by Muni Long...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Too Sweet\" by Hozier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Home\" by Good Neighbours...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Austin\" by Dasha...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Slow It Down\" by Benson Boone...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"vampire\" by Olivia Rodrigo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Igual Que Un Ángel\" by Kali Uchis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Popular\" by The Weeknd...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Rich Baby Daddy (feat. Sexyy Red & SZA)\" by Drake...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"i like the way you kiss me\" by Artemas...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Make You Mine\" by Madison Beer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"CONTIGO\" by KAROL G...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Seven (feat. Latto)\" by Jung Kook...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Whatever\" by Kygo...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"What Was I Made For? [From The Motion Picture \"Barbie\"]\" by Billie Eilish...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during sentiment analysis: The expanded size of the tensor (584) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 584].  Tensor sizes: [1, 514]\n",
      "Searching for \"Flowers\" by Miley Cyrus...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Daylight\" by David Kushner...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"LA FALDA\" by Myke Towers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Is It Over Now? (Taylor's Version) (From The Vault)\" by Taylor Swift...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Standing Next to You\" by Jung Kook...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "track_data = []\n",
    "for i, track in all_tracks.iterrows():\n",
    "\n",
    "    song_name = track[\"name\"] #.partition(\" (\")[0]\n",
    "    song_name = track['name'].partition(\" (with\")[0]\n",
    "    song_name = song_name.partition(\" - From\")[0]\n",
    "\n",
    "    artist_name = track[\"artist\"]\n",
    "\n",
    "    try:\n",
    "        track_lyrics = clean_song_lyrics(song_name, artist_name)\n",
    "        stopwords_removed = remove_stopwords_lyrics(track_lyrics)\n",
    "        lemmatized = word_lemmatize(stopwords_removed)\n",
    "\n",
    "        sentiment_scores = get_lyric_sentiment(stopwords_removed, classifiers)\n",
    "\n",
    "        track_info = track.to_dict()\n",
    "        track_info.update(sentiment_scores)\n",
    "\n",
    "        track_info[\"lyrics\"] = track_lyrics\n",
    "        track_info[\"stopwords_removed\"] = stopwords_removed\n",
    "        track_info[\"lemmatized\"] = lemmatized\n",
    "\n",
    "        track_data.append(track_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing track {track['name']} by {track['artist']}: {e}\")\n",
    "\n",
    "df_tracks = pd.DataFrame(track_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tracks = pd.DataFrame(track_data)\n",
    "df_tracks.to_csv(\"../assets/data/all_tracks+lyrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse160",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
