{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84b5033b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0dcf8",
   "metadata": {},
   "source": [
    "Next, we implement principal component analysis (PCA) on a comprehensive dataset comprising a range of musical features.\n",
    "\n",
    "First, we create a table from the `df_final` dataframe by extracting specific columns that facilitate our analysis. These columns consist of acousticness, danceability, energy, speechiness, tempo, and valence of each track, as well as emotional features such as sadness, joy, love, anger, fear, and surprise. In addition, the table includes the track name and flags for both negative and positive sentiments.\n",
    "\n",
    "We then perform PCA on the data in the table and apply it to generate a biplot depicting the relationship between the features and tracks. This biplot quickly reveals any discernible patterns and clusters within the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SMALL = df_final[['acousticness', 'danceability', 'energy', 'speechiness', \n",
    "                    'tempo', 'valence', 'sadness', 'joy', 'love', 'anger', \n",
    "                    'fear', 'surprise', 'track_name', 'NEGATIVE', 'POSITIVE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86911cfb",
   "metadata": {},
   "source": [
    "To process the data, the code employs the **PCA** and **StandardScaler** modules from the `sklearn` *decomposition* and *preprocessing* libraries. Specifically, the $X_i$ variable is used to choose the first 12 columns from the subset of data mentioned above, while the track_name column is chosen as the target variable. Next, the StandardScaler standardizes the $X_i$ data.\n",
    "\n",
    "PCA is applied to the standardized data, $X_{st}$, using the PCA module, and the resulting loadings and eigenvalues are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27baa0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from bioinfokit.visuz import cluster\n",
    "\n",
    "X_i = X_SMALL.iloc[:,0:12]\n",
    "target = X_SMALL['track_name'].to_numpy()\n",
    "X_st =  StandardScaler().fit_transform(X_i)\n",
    "pca_out = PCA().fit(X_st)\n",
    "\n",
    "# component loadings\n",
    "loadings = pca_out.components_\n",
    "\n",
    "# get eigenvalues (variance explained by each PC)  \n",
    "pca_out.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9992a",
   "metadata": {},
   "source": [
    "Next, the following code uses the `PCA()` function to calculate the PCA scores of the standardized data set, $X_{st}$. \n",
    "\n",
    "A biplot is generated using the cluster module from the bioinfokit library. The biplot is based on the PCA scores and loadings, and the column names of the $X_i$ data frame are used as labels for the plot. The variance explained by the first two principal components are also displayed on the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87ed9d",
   "metadata": {
    "tags": [
     "hide-input",
     "code-fold"
    ]
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# get biplot\n",
    "pca_scores = PCA().fit_transform(X_st)\n",
    "cluster.biplot(cscore=pca_scores, loadings=loadings, labels=X_i.columns.values, \n",
    "               var1=round(pca_out.explained_variance_ratio_[0]*100, 2),\n",
    "               var2=round(pca_out.explained_variance_ratio_[1]*100, 2), #colorlist=target,\n",
    "               show=True,dim=(10,5),dotsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46331b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Then, I assigned the resulting column names to the variable `cols_pca` using a list comprehension. Using the PCA scores, column names, and the original index from $X_i$, I created a new pandas DataFrame called `df_pca`. The first three rows of this new DataFrame is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA()\n",
    "pca_scores = PCA().fit_transform(X_st)\n",
    "cols_pca = [f'PC{i}' for i in range(1, pca_out.n_components_+1)]\n",
    "df_pca = pd.DataFrame(pca_scores, columns=cols_pca, index=X_i.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6615bc1a",
   "metadata": {
    "tags": [
     "hide-code"
    ]
   },
   "outputs": [],
   "source": [
    "df_pca.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60ec7c2d",
   "metadata": {},
   "source": [
    "The variance ratios for the PCA output and the cumulative sum of the explained variance ratios are printed below. Specifically, the array displayed represents the amount of variability explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73733906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_out.explained_variance_ratio_)\n",
    "print('----')\n",
    "print(pca_out.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fddeee4",
   "metadata": {},
   "source": [
    "\n",
    "The loading vectors help visualize the relationship between the original variables and their respective components. These vectors represent the weights of the variables within a mathematical equation used to generate the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc423252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = pd.DataFrame(pca_out.components_.T, columns=df_pca.columns, index=X_i.columns)\n",
    "df_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "130cd4cd",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Spotify Songs - Similarity Search\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Below, we create a query to retrieve similar elements based on Euclidean distance. In mathematics, the Euclidean distance between two points is the length of the line segment between the two points. In this sense, the closer the distance is to 0, the more similar the songs are.\n",
    "\n",
    "\n",
    "\n",
    "#### [KNN Algorithm](https://www.kaggle.com/code/leomauro/spotify-songs-similarity-search/notebook)\n",
    "\n",
    "\n",
    "To obtain a string search for a song, utilize the `getMusicName` function shown below, which returns the artist and song name. \n",
    "\n",
    "The k-Nearest Neighbors (KNN) algorithm searches for k similar elements based on a query point at the center within a predefined radius. We execute the KNN algorithm using the `knnQuery` function defined below, which takes a query point, a set of characteristic points, and a value for k. It computes the sum of squared differences between each data and query point, followed by the calculation of the Euclidean distance between them. The function then arranges the points by distance and returns the k closest and farthest points.\n",
    "\n",
    "The `querySimilars` function then removes the query point and executes the KNN algorithm on the remaining points, returning the k most similar points to the query point based on the specified columns, function, and parameter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a song string search\n",
    "def getMusicName(elem):\n",
    "    return f\"{elem['artist']} - {elem['name']}\"\n",
    "\n",
    "def knnQuery(queryPoint, arrCharactPoints, k):\n",
    "    queryVals = queryPoint.tolist()\n",
    "    distVals = []\n",
    "    \n",
    "    # Copy of dataframe indices and data\n",
    "    tmp = arrCharactPoints.copy(deep = True)  \n",
    "    for index, row in tmp.iterrows():\n",
    "        feat = row.values.tolist()\n",
    "        \n",
    "        # Calculate sum of squared differences\n",
    "        ssd = sum(abs(feat[i] - queryVals[i]) ** 2 for i in range(len(queryVals)))\n",
    "        \n",
    "        # Get euclidean distance\n",
    "        distVals.append(ssd ** 0.5)\n",
    "        \n",
    "    tmp['distance'] = distVals\n",
    "    tmp = tmp.sort_values('distance')\n",
    "    \n",
    "    # K closest and furthest points\n",
    "    return tmp.head(k).index, tmp.tail(k).index\n",
    "\n",
    "\n",
    "# Execute KNN removing the query point\n",
    "def querySimilars(df, columns, idx, func, param):\n",
    "    arr = df[columns].copy(deep = True)\n",
    "    queryPoint = arr.loc[idx]\n",
    "    arr = arr.drop([idx])\n",
    "    return func(queryPoint, arr, param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efadcb53",
   "metadata": {},
   "source": [
    "**KNN Query Example.** \n",
    "\n",
    "We now establish a function that creates customized query points and alters the data columns, allowing for further exploration of various options. To illustrate, the code snippet below chooses a particular group of song features and then seeks out the top k values within that feature set that are equal to one.\n",
    "\n",
    "To begin, we create a scaler utilizing the preprocessing library from *sklearn*. It's worth noting that all the feature values fall within the range of 0 and 1, except for loudness. As a result, we need to scale loudness to conform to the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d32eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# scale loudness to fit the same range [0, 1]\n",
    "loudness2 = df[\"loudness\"].values\n",
    "loudness_scaled=scaler.fit_transform(loudness2.reshape(-1, 1))\n",
    "df['loudness_scaled'] = loudness_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e4d1cf",
   "metadata": {},
   "source": [
    "Let's search for  $k=3$  similar songs to a query point $\\textrm{songIndex} = 6$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select song and column attributes\n",
    "songIndex = 4 # query point\n",
    "columns = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', \n",
    "           'loudness_scaled', 'tempo', \n",
    "           'speechiness', 'valence']\n",
    "\n",
    "# Set query parameters\n",
    "func, param = knnQuery,3\n",
    "\n",
    "# Implement query\n",
    "response = querySimilars(df, columns, songIndex, func, param)\n",
    "\n",
    "print(\"---- Query Point ----\")\n",
    "print(getMusicName(df.loc[songIndex]))\n",
    "print('---- k = 3 similar songs ----')\n",
    "for track_id in response[0]:\n",
    "    track_name = getMusicName(df.loc[track_id])\n",
    "    print(track_name)\n",
    "print('---- k = 3 nonsimilar songs ----')\n",
    "for track_id in response[1]:\n",
    "    track_name = getMusicName(df.loc[track_id])\n",
    "    print(track_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fa8aa95",
   "metadata": {},
   "source": [
    "The code below implements the same idea as above, but queries each track in a given playlist instead of a single defined query point.\n",
    "\n",
    "To keep track of the number of songs that are similar and those that are not, we use two dictionaries: similar_count\" and \"nonsimilar_count\". To do this, we create a loop that goes through the data, running the `querySimilars` function on each track. A loop then processes \"similar\" and \"non-similar\" songs from the results of the query, stored in the \"response\" variable. If a \"similar\" song is found, its name is retrieved using the `getMusicName` function. The song's name is then added to the \"similar_count\" dictionary with a count of 1, or incremented if it already exists.\n",
    "\n",
    "The same process is repeated for the \"non-similar\" songs, except the count is added to the \"nonsimilar_count\" dictionary instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f41e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_count = {} # Similar songs count\n",
    "nonsimilar_count = {} # Non-similar songs count\n",
    "\n",
    "for track_index in df.index:\n",
    "    # Implement query\n",
    "    response = querySimilars(df, columns, track_index, func, param)\n",
    "    \n",
    "    # Get similar songs\n",
    "    for similar_index in response[0]:\n",
    "        track = getMusicName(df.loc[similar_index])\n",
    "        if track in similar_count:\n",
    "            similar_count[track] += 1\n",
    "        else:\n",
    "            similar_count[track] = 1\n",
    "    \n",
    "    # Get non-similar songs\n",
    "    for nonsimilar_index in response[1]:\n",
    "        track = getMusicName(df.loc[nonsimilar_index])\n",
    "        if track in nonsimilar_count:\n",
    "            nonsimilar_count[track] += 1\n",
    "        else:\n",
    "            nonsimilar_count[track] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7143e67e",
   "metadata": {},
   "source": [
    "Next, we display both the non-similar and similar songs with their respective track name and count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "nonsimilar = dict(sorted(nonsimilar_count.items(), key=lambda item: item[1], reverse=True))\n",
    "print('---- NON SIMILAR SONG COUNTS ----')\n",
    "for track_name, track_count in nonsimilar.items():\n",
    "    if track_count >= 8:\n",
    "        print(track_name, ':', track_count)\n",
    "\n",
    "similar = dict(sorted(similar_count.items(), key=lambda item: item[1], reverse=True))\n",
    "print('\\n---- SIMILAR SONG COUNTS ----')\n",
    "for track_name, track_count in similar.items():\n",
    "    if track_count >= 5:\n",
    "        print(track_name, ':', track_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6a19f",
   "metadata": {},
   "source": [
    "As shown above, the code snippet arranges the \"nonsimilar_count\" dictionary in a descending sequence, followed by presenting the tracks with the highest non-similar query counts. We repeat the same process for songs that are similar from the \"similar_count\" dictionary.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea412d4b",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "## Organized Songs in a Playlist\n",
    "\n",
    "\n",
    "Below, we import the Python pandas, matplotlib.pyplot, and sklearn libraries to our project. These tools help us perform various operations such as clustering, decomposition, and data visualization.\n",
    "\n",
    "We then obtain a list of songs including their name and various attributes such as acousticness, danceability, energy, instrumentalness, liveness, speechiness, tempo, valence, and loudness. Next, we gather helpful insights about these songs using the' describe' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, decomposition\n",
    "\n",
    "songs = df[['name','acousticness', 'danceability', 'energy', 'instrumentalness', \n",
    "            'liveness', 'speechiness', 'tempo', 'valence',  'loudness_scaled']]\n",
    "songs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e5750",
   "metadata": {},
   "source": [
    "Extracting the song labels from the dataset is the first crucial step. Then, we must select the appropriate features that will serve as inputs for the Affinity Propagation clustering algorithm from the *scikit-learn* library. During the clustering process, a preference value of -200 is used to ensure optimal performance. Once the data is inputted, the algorithm is trained to achieve the desired outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda942e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = songs.values[:,0]\n",
    "X = songs.values[:,1:10]\n",
    "kmeans = cluster.AffinityPropagation(preference=-200)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d295b49",
   "metadata": {},
   "source": [
    "The script below utilizes a dictionary called \"predictions\" to keep track of the outcomes of a comparison process between two lists: \"kmeans.predict(X)\" and \"labels\". For each new value, a unique key is generated in the dictionary with the corresponding value from the \"labels\" list appended to the key's list of values.\n",
    "\n",
    "After sorting all values into their designated keys, we proceed to display each key alongside its relevant values. The output displays each category and the corresponding songs it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for p,n in zip(kmeans.predict(X),labels):\n",
    "    if not predictions.get(p):\n",
    "        predictions[p] = []\n",
    "        \n",
    "    predictions[p] += [n]\n",
    "\n",
    "for p in predictions:\n",
    "    print(\"Category\",p)\n",
    "    print(\"-----\")\n",
    "    for n in predictions[p]:\n",
    "        print(n)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c28167",
   "metadata": {},
   "source": [
    "The script successfully categorized the playlist into 6 distinct groups based on shared features, resulting in a diverse selection of songs within each category."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13af5dac",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "#### K Means Clustering\n",
    "\n",
    "\n",
    "Using K Means clustering, we choose to [break the playlist into 3 smaller playlists](https://github.com/ankushbharadwaj/reorganize-my-spotify-playlist/blob/master/reorder%20my%20spotify%20playlist.ipynb).\n",
    "\n",
    "As shown below, we employ the KMeans algorithm, obtained from the *sklearn.cluster* library, to cluster a collection of songs into distinct categories based on track features, such as their energy levels and sound qualities. Using three clusters, we apply this algorithm on the track features from the \"playlist_tracks\" subset of data, dropping the \"artist\" and \"name\" columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "playlist_tracks = df[['artist','name','acousticness','danceability','energy',\n",
    "                      'liveness', 'instrumentalness','speechiness','valence']]\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "kmeans.fit(playlist_tracks.drop(['artist', 'name'], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6be01a",
   "metadata": {},
   "source": [
    "Additionally, we import the \"seaborn\" library and use the \"%matplotlib inline\" command to generate useful plots displayed inline. Below, we generate a count plot with x-axis values set to a list of groups, displayed as string representations, produced by the k-means clustering algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=[str(group) for group in kmeans.labels_], color = 'lightblue')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e04e714",
   "metadata": {},
   "source": [
    "#### Visualizing the Clusters\n",
    "\n",
    "\n",
    "Moving forward, let's look at differences in the audio features of each group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(playlist_tracks.drop(['artist', 'name'], axis = 1))\n",
    "scaled_data = scaler.transform(playlist_tracks.drop(['artist', 'name'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components =2)\n",
    "pca.fit(scaled_data)\n",
    "data_pca = pca.transform(scaled_data)\n",
    "\n",
    "plt.scatter(data_pca[:,0], data_pca[:,1], c = list(kmeans.labels_), cmap = 'Paired')\n",
    "plt.xlabel('PC1: {:.3f}'.format(pca.explained_variance_ratio_[0]), size = 15)\n",
    "plt.ylabel('PC2: {:.3f}'.format(pca.explained_variance_ratio_[1]), size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c21c6",
   "metadata": {
    "tags": [
     "hide-code"
    ]
   },
   "outputs": [],
   "source": [
    "plot = sns.scatterplot(x=data_pca[:,0], y=data_pca[:,1], hue = list(kmeans.labels_),\n",
    "                       alpha = 0.66, \n",
    "                       #size = data_pca[:,1], sizes = (50,200),\n",
    "                       palette = 'viridis', edgecolor = 'black', cmap='Paired')\n",
    "plot.set_title('PCA ANALYSIS', size = 16, weight='bold')\n",
    "\n",
    "plt.xlabel('PC1: {:.3f}'.format(pca.explained_variance_ratio_[0]), size = 15)\n",
    "plt.ylabel('PC2: {:.3f}'.format(pca.explained_variance_ratio_[1]), size = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_tracks['group'] = list(kmeans.labels_)\n",
    "playlist_tracks = playlist_tracks.astype({'group': str})\n",
    "\n",
    "means = pd.DataFrame(index = range(0,3), \n",
    "                    columns = list(playlist_tracks[playlist_tracks['group'] == '0'].describe().loc['mean'].index))\n",
    "means.iloc[0] = playlist_tracks[playlist_tracks['group'] == '0'].describe().loc['mean']\n",
    "means.iloc[1] = playlist_tracks[playlist_tracks['group'] == '1'].describe().loc['mean']\n",
    "means.iloc[2] = playlist_tracks[playlist_tracks['group'] == '2'].describe().loc['mean']\n",
    "means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
