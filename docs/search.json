[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP Song Lyrics",
    "section": "",
    "text": "Extracting Spotify Data\nGetting started, we want to extract data for a set of tracks within one of Spotify‚Äôs top-featured playlists. Leveraging the Spotify Web API, we can seamlessly obtain detailed data for a song, such as the performing artist, the album it belongs to, its release date, popularity, and audio features like danceability, energy, and tempo.\nPython libraries like spotipy offer a user-friendly way to interact with the Spotify API, offering a range of functions that streamline tasks like API authentication, retrieving playlist data, and obtaining information about any given song.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "index.html#extracting-spotify-data",
    "href": "index.html#extracting-spotify-data",
    "title": "NLP Song Lyrics",
    "section": "",
    "text": "Accessing the Spotify Web API\nTo access data from Spotify, we import the spotipy library and the SpotifyClientCredentials module. Additionally, we utilize the pandas package for data manipulation and display. In order to authenticate our access to the Spotify API, we must provide our client ID and client secret to a client credentials manager. Once authenticated, we can use the spotipy module to interact with the Spotify API and retrieve data.\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nclient_id = \"xxx\"\nclient_secret = \"xxx\"\nmy_auth = SpotifyClientCredentials(client_id, client_secret)\nsp = spotipy.Spotify(auth_manager=my_auth)\n\n\nSpotify‚Äôs Featured Playlists\nLet‚Äôs take a look at the popular Spotify playlists. Below, the code retrieves a range of Spotify playlists and generates a dataframe containing details for each playlist, such as its name, ID, description, thumbnail, and total number of tracks. The resulting dataframe is displayed as an HTML table.\n\nusername = \"spotify\"\nmy_playlists = sp.user_playlists(username)\n\n\n\n\n\n\n\n\nthumbnail\nplaylist_name\nplaylist_id\ndescription\ntotal\n\n\n\n\n0\n\nToday‚Äôs Top Hits\n37i9dQZF1DXcBWIGoYBM5M\nOlivia Rodrigo is on top of the Hottest 50!\n50\n\n\n1\n\nRapCaviar\n37i9dQZF1DX0XUsuxWHRQd\nNew music from Future and Metro Boomin.\n50\n\n\n2\n\nHot Country\n37i9dQZF1DX1lVhptIYRda\nToday's top country hits. Cover: Kenny Chesney\n50\n\n\n3\n\nViva Latino\n37i9dQZF1DX10zKzsJ2jva\nToday's top Latin hits, elevando nuestra m√∫sica. Shakira tiene algo especial para ti. ‚ù§Ô∏è‚Äçüî•\n50\n\n\n4\n\nNew Music Friday\n37i9dQZF1DX4JAvHpjipBk\nNew music from Shakira, Metro Boomin & Future, Tyla, Olivia Rodrigo, and more!\n100\n\n\n\n\n\n\n\n\n\n\nExtracting Tracks From a Playlist\nThe script enables the compilation of song and artist data from any Spotify playlist through its URI. The get_playlist_tracks method returns a complete list of track IDs and corresponding artists from the selected playlist.\n\ndef get_playlist_tracks(playlist_URI):\n    tracks = []\n    results = sp.playlist_tracks(playlist_URI)\n    tracks = results[\"items\"]\n    while results[\"next\"]:\n        results = sp.next(results)\n        tracks.extend(results[\"items\"])\n    return tracks\n\n\nExtracting Features from Tracks\nNext, we utilize Spotify‚Äôs API to extract further details about each song within the playlist. We obtain metadata such as the track name, the artist it‚Äôs sung by, the album it belongs to, the release date, and track features such as danceability, tempo, and popularity.\n\n\n\n\n\n\n\n\n\n\nname\ntrack_id\nalbum\nartist\nartist_id\nrelease_date\nlength\npopularity\nartist_pop\nartist_genres\n...\ninstrumentalness\nliveness\nloudness\nspeechiness\ntempo\nvalence\nkey\nmode\ntime_signature\nplaylist\n\n\n\n\n0\nBeautiful Things\n6tNQ70jh4OwmPGpYy6R2o9\nBeautiful Things\nBenson Boone\n22wbnEMDvgVIAGdFeek6ET\n2024-01-18\n180304\n100\n84\n['singer-songwriter pop']\n...\n0.0\n0.140\n-5.692\n0.0603\n105.029\n0.219\n10\n1\n3\nToday‚Äôs Top Hits\n\n\n1\nobsessed\n6tNgRQ0K2NYZ0Rb9l9DzL8\nGUTS (spilled)\nOlivia Rodrigo\n1McMsnEElThX1knmY4oliG\n2024-03-22\n170541\n79\n89\n['pop']\n...\n0.0\n0.305\n-5.094\n0.0915\n119.985\n0.697\n8\n1\n4\nToday‚Äôs Top Hits\n\n\n\n\n2 rows √ó 23 columns\n\n\n\n\n\n\nTop Artist Genres",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "index.html#natural-language-processing",
    "href": "index.html#natural-language-processing",
    "title": "NLP Song Lyrics",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nUsing the data gathered from the Spotify API, we now want to extract and process the lyrics for each song. This is accomplished through scraping textual information, namely lyrical data, from the Genius Lyrics website. Following extraction, the lyrics are thoroughly processed and cleaned before undergoing sentiment analysis.\n\n\nScraping the Web\nTo get started, the script below imports lyricsgenius, a fundamental package libary allowing for web scraping of the Genius Lyrics website to retrieve the lyrics of any given song. Through the initialization of the genius variable, one can access the Genius API and retrieve the lyrics of any given song, such as ‚ÄúToo Many Nights‚Äù by Metro Boomin.\n\nimport lyricsgenius\n\ngenius = lyricsgenius.Genius(\"epFCxujgBe-Y6WrkZedI8kerKxiCpR6Rh0DAHYNlKDf9B4H1nXTdZIkj7krNUHVV\")\nsong = genius.search_song(\"Too Many Nights\", \"Metro Boomin\")\n\nSearching for \"Too Many Nights\" by Metro Boomin...\nDone.\n\n\nWe define a function to efficiently fetch the lyrics of a song using the given track name and artist. Once the lyrics are retrieved, the next step is to pre-process the textual data. This involves a cleansing process to eliminate profanity, specific characters, and patterns that may hinder the overall readability. The resulting output is a list of cleaned lyrics with all non-ASCII characters removed.\n\n\nPre-Processing Text Data\nThe Python script contains various functions optimized for efficiently cleaning song lyrics. The text pre-processing procedure involves the following main steps:\n\nLanguage Detection\nExpanding Contractions\nConverting Text to Lowercase\nSpell Checking + Censoring\nRemoving Punctuations\nTokenization\n\n\ndef clean_song_lyrics(song_name, artist_name):\n    genius_lyrics = get_song_lyrics(song_name, artist_name)\n    lyrics_en = detect_and_translate(genius_lyrics, \"en\")\n\n    no_contract = [contractions.fix(word).lower() for word in lyrics_en.split()]\n    corrected = spell_check(\" \".join(no_contract)) # &lt;4&gt; # Spell Check + Censor\n    censored = profanity.censor(\" \".join(corrected), censor_char=\"\")\n    no_punct = remove_punctuation(censored)\n\n    tokenized = word_tokenize(no_punct) # Tokenize\n    return [i.encode(\"ascii\", \"ignore\").decode() for i in tokenized]\n\nThe clean_song_lyrics function is designed to simplify the processing of lyrics for a specific song and artist. The function extracts the lyrics from the Genius database and performs a series of modifications, including expanding contractions, removing repetitive phrases, and converting the text to lowercase. It also ensures that the spelling is correct and eliminates any profanity. The end result is a cleaned set of lyrics, tokenized and encoded as a list of words.\n\nFurther Text Cleaning\nRemoving Stop Words: We employ the Natural Language Toolkit (NLTK) library and its WordNetLemmatizer tool to filter out stopwords. By removing frequently used words like ‚Äúthe,‚Äù ‚Äúand,‚Äù or ‚Äúof,‚Äù the resulting text becomes more concise, enabling a more thorough examination of the lyrics and their underlying message.\nLemmatization: We perform lemmatization on a set of words using the WordNetLemmatizer class from the NLTK library. Lemmatization helps to standardize words and reduce their complexity. Our function specifically targets verbs and transforms different variations of the same verb into its most basic form.\nSee the full documentation here.\n\n\nExploratory text analysis\n\n\n\n\n\n\n\n\n\n\nname\nartist\nlyrics\nstopwords_removed\nlemmatized\n\n\n\n\n0\nBeautiful Things\nBenson Boone\n['for', 'a', 'while', 'there', 'it', 'was', 'rough'...\n['rough', 'lately', 'better', 'last', 'four', 'cold...\n['rough', 'lately', 'well', 'last', 'four', 'cold',...\n\n\n1\nobsessed\nOlivia Rodrigo\n['law', 'dada', 'data', 'lambda', 'law', 'dada', 'd...\n['law', 'dada', 'data', 'lambda', 'law', 'dada', 'd...\n['law', 'dada', 'data', 'lambda', 'law', 'dada', 'd...\n\n\n2\nwe can't be friends (wait for your love)\nAriana Grande\n['i', 'did', 'not', 'think', 'you', 'would', 'under...\n['think', 'would', 'understand', 'could', 'ever', '...\n['think', 'would', 'understand', 'could', 'ever', '...\n\n\n\n\n\n\n\n\n\nTerm Frequency Analysis\nLet‚Äôs examine the most frequent words. Plotting the frequency distribution helps to determine the occurrence of the most common terms in our lyrical corpus. Initially, we transform our individual lists of tokenized lyrics into a comprehensive list of iterable tokens that encompasses all the lyrics collectively.\n\nfrom nltk.probability import FreqDist\n\nall_words = []\nfor words in df_lyrics['lemmatized']:\n    res = words.replace(\"'\", \"\").strip('][').split(', ')\n    all_words += res\n\nmostcommon = FreqDist(all_words).most_common(30)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "index.html#sentiment-analysis",
    "href": "index.html#sentiment-analysis",
    "title": "NLP Song Lyrics",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nSubsequently, the process involves the implementation of pipeline classes to carry out predictions using models accessible in the Hub. The code imports and employs multiple transformer models specifically designed for text classification and sentiment analysis. Specifically, the following procedure creates three distinct pipelines, each equipped with different models that facilitate the assessment of emotions and sentiment in textual content.\n\nimport transformers\nfrom transformers import pipeline\n\n# Initialize Genius API and sentiment classifiers\nclassifiers = [\n    pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True),\n    pipeline(\"text-classification\", model='cardiffnlp/twitter-roberta-base-sentiment', return_all_scores=True),\n    pipeline(\"sentiment-analysis\", return_all_scores=True)\n]\n\nThe get_lyric_sentiment function takes in pre-processed lyrics as input and produces a dictionary of sentiment scores. It leverages three distinct classifiers to calculate the scores and aggregates them into a final result. For instance, one of these classifiers is the distilbert-base-uncased-emotion model, specifically trained to detect ‚Äúemotions in texts such as sadness, joy, love, anger, fear, and surprise‚Äù.\n\n# Function to perform sentiment analysis\ndef get_lyric_sentiment(lyrics, classifiers):\n    text = \" \".join(lyrics)\n    scores = {}\n    for classifier in classifiers:\n        try:\n            predictions = classifier(text, truncation=True)\n            for prediction in predictions[0]:\n                scores[prediction[\"label\"]] = prediction[\"score\"]\n        except Exception as e:\n            print(f\"Error during sentiment analysis: {e}\")\n    return scores",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "index.html#putting-it-all-together",
    "href": "index.html#putting-it-all-together",
    "title": "NLP Song Lyrics",
    "section": "Putting it All Together",
    "text": "Putting it All Together\nTo summarize, the code efficiently collects data and performs text analysis on every song in a playlist. Specifically, it systematically processes a list of tracks and corresponding artists while simultaneously conducting a thorough cleaning procedure on the lyrics. Additionally, the program computes a sentiment score for each song based on the lyrics, indicating whether the lyrics are positive, negative, or neutral.\n\n\n\n\n\n\n\n\n\n\nname\nalbum\nartist\nrelease_date\nlength\npopularity\nartist_pop\nartist_genres\nacousticness\ndanceability\n...\njoy\nlove\nanger\nfear\nsurprise\nLABEL_0\nLABEL_1\nLABEL_2\nNEGATIVE\nPOSITIVE\n\n\n\n\n0\nBeautiful Things\nBeautiful Things\nBenson Boone\n2024-01-18\n180304\n100\n84\n['singer-songwriter pop']\n0.1510\n0.472\n...\n0.000454\n0.000210\n0.000819\n0.996218\n0.000817\n0.033695\n0.346160\n0.620145\n0.012751\n0.987248\n\n\n1\nobsessed\nGUTS (spilled)\nOlivia Rodrigo\n2024-03-22\n170541\n79\n89\n['pop']\n0.0219\n0.683\n...\n0.000476\n0.000644\n0.000680\n0.003106\n0.000368\n0.301736\n0.594620\n0.103644\n0.994116\n0.005884\n\n\n2\nwe can't be friends (wait for your love)\neternal sunshine\nAriana Grande\n2024-03-08\n228639\n88\n94\n['pop']\n0.0716\n0.647\n...\n0.992336\n0.001261\n0.001286\n0.000605\n0.000460\n0.063339\n0.507926\n0.428735\n0.996296\n0.003704\n\n\n\n\n3 rows √ó 32 columns\n\n\n\n\nIn summary, the above code aims to collect and refine song lyrics by eliminating stopwords and conducting lemmatization. Subsequently, it employs pre-trained models for sentiment analysis to determine the prevailing emotion conveyed in the lyrics. Finally, the program compiles all this information into a dataframe for further analysis.\n\n\n\n\n\n\n\n\n\n\n\nTwitter-roBERTa-base for Sentiment Analysis\nNow, we present a graphical representation of the results obtained from the roBERTa-base model ‚Äútrained on roughly 58 million tweets and fine-tuned for sentiment analysis using the TweetEval benchmark‚Äù (EMNLP 2020). According to the TweetEval reference paper and official Github repository, the resulting labels 0, 1, and 2 correspond to Negative, Neutral, and Positive, respectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "index.html#correlations-matrix",
    "href": "index.html#correlations-matrix",
    "title": "NLP Song Lyrics",
    "section": "Correlations Matrix",
    "text": "Correlations Matrix\nAfter completing the initial data analysis, we proceed with generating the Pearson correlations matrix using the Pandas command df.corr(). Subsequently, we visualize the matrix using the seaborn heatmap, providing a detailed understanding of the relationships between the various variables in our dataset.\n\ntrack_sentiment_df = df_final[['name', 'artist',\n           'acousticness', 'danceability', 'energy', 'instrumentalness', \n           'loudness', 'speechiness', 'tempo', 'valence', \n           'sadness', 'joy', 'love', 'anger', 'fear', 'surprise',\n           'LABEL_0', 'LABEL_1', 'LABEL_2', 'NEGATIVE', 'POSITIVE']]\n\n# Find the pearson correlations matrix\ncorr = track_sentiment_df.corr(method = 'pearson')\n\n\n\n\n\n\n\n\n\n\nThe code below produces a scatterplot that showcases the correlation between energy and fear. The x-axis represents the energy value, while the y-axis represents the fear sentiment. The size of each data point corresponds to the label indicating the neutral sentiment level, and its color represents the valence value. Moreover, each bubble contains its energy value within, allowing for a straightforward interpretation of the data.\n\n\nText(0.5, 1.0, 'Fear vs. Energy')\n\n\n\n\n\n\n\n\n\nSimilarly, the scatterplot presented above utilizes the track sentiment data, comparing the energy and fear levels of the tracks based on valence and size.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "index.html#pca-analysis",
    "href": "index.html#pca-analysis",
    "title": "NLP Song Lyrics",
    "section": "PCA Analysis",
    "text": "PCA Analysis\nNext, we implement principal component analysis (PCA) on a comprehensive dataset comprising a range of musical features.\nFirst, we create a table from the df_final dataframe by extracting specific columns that facilitate our analysis. These columns consist of acousticness, danceability, energy, speechiness, tempo, and valence of each track, as well as emotional features such as sadness, joy, love, anger, fear, and surprise. In addition, the table includes the track name and flags for both negative and positive sentiments.\nWe then perform PCA on the data in the table and apply it to generate a biplot depicting the relationship between the features and tracks. This biplot quickly reveals any discernible patterns and clusters within the dataset.\n\nX_SMALL = df_final[['acousticness', 'danceability', 'energy', 'speechiness', \n                    'tempo', 'valence', 'sadness', 'joy', 'love', 'anger', \n                    'fear', 'surprise', 'name', 'NEGATIVE', 'POSITIVE']]\n\nTo process the data, the code employs the PCA and StandardScaler modules from the sklearn decomposition and preprocessing libraries. Specifically, the \\(X_i\\) variable is used to choose the first 12 columns from the subset of data mentioned above, while the track_name column is chosen as the target variable. Next, the StandardScaler standardizes the \\(X_i\\) data.\nPCA is applied to the standardized data, \\(X_{st}\\), using the PCA module, and the resulting loadings and eigenvalues are saved.\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom bioinfokit.visuz import cluster\n\nX_i = X_SMALL.iloc[:,0:12]\ntarget = X_SMALL['name'].to_numpy()\nX_st =  StandardScaler().fit_transform(X_i)\npca_out = PCA().fit(X_st)\n\n# component loadings\nloadings = pca_out.components_\n\n# get eigenvalues (variance explained by each PC)  \npca_out.explained_variance_\n\narray([2.73922470e+00, 1.81872809e+00, 1.55361774e+00, 1.29202669e+00,\n       1.11165244e+00, 1.03938832e+00, 8.09497570e-01, 6.42936686e-01,\n       5.17927574e-01, 4.14876283e-01, 3.05021862e-01, 8.89034026e-15])\n\n\nNext, the following code uses the PCA() function to calculate the PCA scores of the standardized data set, \\(X_{st}\\).\nA biplot is generated using the cluster module from the bioinfokit library. The biplot is based on the PCA scores and loadings, and the column names of the \\(X_i\\) data frame are used as labels for the plot. The variance explained by the first two principal components are also displayed on the plot.\n\n# get biplot\npca_scores = PCA().fit_transform(X_st)\ncluster.biplot(cscore=pca_scores, loadings=loadings, labels=X_i.columns.values, \n               var1=round(pca_out.explained_variance_ratio_[0]*100, 2),\n               var2=round(pca_out.explained_variance_ratio_[1]*100, 2), #colorlist=target,\n               show=True,dim=(10,5),dotsize=16)\n\n\n\n\n\n\n\n\nThen, I assigned the resulting column names to the variable cols_pca using a list comprehension. Using the PCA scores, column names, and the original index from \\(X_i\\), I created a new pandas DataFrame called df_pca. The first three rows of this new DataFrame is shown below.\n\npca_scores = PCA().fit_transform(X_st)\ncols_pca = [f'PC{i}' for i in range(1, pca_out.n_components_+1)]\ndf_pca = pd.DataFrame(pca_scores, columns=cols_pca, index=X_i.index)\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\n\n\n\n\n0\n1.968748\n-0.421134\n0.638886\n2.608245\n-1.238485\n0.684002\n-0.310952\n-0.698681\n-0.991042\n0.091204\n-0.897456\n1.600704e-07\n\n\n1\n-0.308694\n-1.537003\n-1.668544\n-1.290715\n-1.273783\n-0.477774\n1.070345\n-0.053149\n-1.130636\n0.337365\n-0.805948\n-6.780730e-08\n\n\n2\n-0.819077\n0.142560\n1.177381\n0.109127\n0.103044\n-0.814402\n-1.344701\n-0.562481\n-0.607264\n0.545114\n0.293676\n-9.294006e-08\n\n\n\n\n\n\n\n\nThe variance ratios for the PCA output and the cumulative sum of the explained variance ratios are printed below. Specifically, the array displayed represents the amount of variability explained by each component.\n\nprint(pca_out.explained_variance_ratio_)\nprint('----')\nprint(pca_out.explained_variance_ratio_.cumsum())\n\n[2.23703351e-01 1.48529460e-01 1.26878782e-01 1.05515513e-01\n 9.07849493e-02 8.48833796e-02 6.61089682e-02 5.25064960e-02\n 4.22974186e-02 3.38815631e-02 2.49101188e-02 7.26044455e-16]\n----\n[0.22370335 0.37223281 0.49911159 0.60462711 0.69541206 0.78029544\n 0.8464044  0.8989109  0.94120832 0.97508988 1.         1.        ]\n\n\nThe loading vectors help visualize the relationship between the original variables and their respective components. These vectors represent the weights of the variables within a mathematical equation used to generate the principal components.\n\ndf_weights = pd.DataFrame(pca_out.components_.T, columns=df_pca.columns, index=X_i.columns)\ndf_weights\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\n\n\n\n\nacousticness\n0.408035\n-0.146674\n0.184069\n-0.305044\n-0.168769\n0.307214\n-0.125811\n0.160301\n0.295463\n-0.379955\n0.537596\n6.553177e-09\n\n\ndanceability\n-0.422117\n0.187035\n-0.143083\n-0.057866\n-0.124999\n0.336009\n0.148793\n0.440103\n0.011646\n0.501877\n0.408216\n-2.788028e-09\n\n\nenergy\n-0.368829\n-0.082644\n-0.368905\n0.203881\n0.358443\n-0.104268\n-0.138360\n-0.060305\n-0.266860\n-0.479832\n0.464641\n6.133656e-09\n\n\nspeechiness\n-0.284747\n-0.121462\n0.306162\n-0.039564\n-0.115693\n0.214292\n0.771135\n-0.254452\n-0.088982\n-0.286787\n-0.000522\n8.034013e-09\n\n\ntempo\n0.010528\n-0.511718\n-0.003009\n0.187506\n0.339350\n-0.333285\n0.257478\n0.068690\n0.564378\n0.247237\n0.161144\n-1.971415e-08\n\n\nvalence\n-0.394459\n-0.219710\n-0.286606\n-0.140502\n-0.115587\n0.257849\n-0.155436\n0.259498\n0.403342\n-0.335007\n-0.497981\n1.801651e-09\n\n\nsadness\n0.172418\n-0.311660\n-0.428734\n-0.441682\n-0.205517\n-0.264849\n0.229241\n0.127144\n-0.313293\n0.049862\n-0.009728\n-4.608803e-01\n\n\njoy\n-0.373561\n0.048629\n0.557857\n-0.157352\n0.087738\n-0.206966\n-0.276271\n0.047920\n0.050902\n-0.023722\n0.038639\n-6.231547e-01\n\n\nlove\n-0.007973\n0.523938\n-0.342743\n0.064558\n-0.243361\n-0.149107\n0.125627\n-0.390504\n0.492266\n-0.070746\n0.137083\n-2.965285e-01\n\n\nanger\n0.173068\n0.086883\n-0.134633\n-0.219114\n0.673753\n0.524189\n0.056930\n-0.214683\n0.027612\n0.132951\n-0.117798\n-2.932582e-01\n\n\nfear\n0.217856\n-0.150010\n-0.019676\n0.730233\n-0.184948\n0.302261\n0.020816\n0.179569\n-0.087676\n-0.050108\n-0.051883\n-4.746288e-01\n\n\nsurprise\n0.189071\n0.455779\n0.051612\n0.017458\n0.296982\n-0.238771\n0.330834\n0.625207\n0.026619\n-0.299452\n-0.132511\n-8.100685e-03",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "index.html#spotify-songs---similarity-search",
    "href": "index.html#spotify-songs---similarity-search",
    "title": "NLP Song Lyrics",
    "section": "Spotify Songs - Similarity Search",
    "text": "Spotify Songs - Similarity Search\nBelow, we create a query to retrieve similar elements based on Euclidean distance. In mathematics, the Euclidean distance between two points is the length of the line segment between the two points. In this sense, the closer the distance is to 0, the more similar the songs are.\n\nKNN Algorithm\nTo obtain a string search for a song, utilize the getMusicName function shown below, which returns the artist and song name.\nThe k-Nearest Neighbors (KNN) algorithm searches for k similar elements based on a query point at the center within a predefined radius. We execute the KNN algorithm using the knnQuery function defined below, which takes a query point, a set of characteristic points, and a value for k. It computes the sum of squared differences between each data and query point, followed by the calculation of the Euclidean distance between them. The function then arranges the points by distance and returns the k closest and farthest points.\nThe querySimilars function then removes the query point and executes the KNN algorithm on the remaining points, returning the k most similar points to the query point based on the specified columns, function, and parameter.\n\n# Get a song string search\ndef getMusicName(elem):\n    return f\"{elem['artist']} - {elem['name']}\"\n\ndef knnQuery(queryPoint, arrCharactPoints, k):\n    queryVals = queryPoint.tolist()\n    distVals = []\n    \n    # Copy of dataframe indices and data\n    tmp = arrCharactPoints.copy(deep = True)  \n    for index, row in tmp.iterrows():\n        feat = row.values.tolist()\n        \n        # Calculate sum of squared differences\n        ssd = sum(abs(feat[i] - queryVals[i]) ** 2 for i in range(len(queryVals)))\n        \n        # Get euclidean distance\n        distVals.append(ssd ** 0.5)\n        \n    tmp['distance'] = distVals\n    tmp = tmp.sort_values('distance')\n    \n    # K closest and furthest points\n    return tmp.head(k).index, tmp.tail(k).index\n\n\n# Execute KNN removing the query point\ndef querySimilars(df, columns, idx, func, param):\n    arr = df[columns].copy(deep = True)\n    queryPoint = arr.loc[idx]\n    arr = arr.drop([idx])\n    return func(queryPoint, arr, param)\n\nKNN Query Example.\nWe now establish a function that creates customized query points and alters the data columns, allowing for further exploration of various options. To illustrate, the code snippet below chooses a particular group of song features and then seeks out the top k values within that feature set that are equal to one.\nTo begin, we create a scaler utilizing the preprocessing library from sklearn. It‚Äôs worth noting that all the feature values fall within the range of 0 and 1, except for loudness. As a result, we need to scale loudness to conform to the same range.\n\ndf = df_final\n\n\nfrom sklearn import preprocessing \n\nscaler = preprocessing.MinMaxScaler()\n# scale loudness to fit the same range [0, 1]\nloudness2 = df[\"loudness\"].values\nloudness_scaled=scaler.fit_transform(loudness2.reshape(-1, 1))\ndf['loudness_scaled'] = loudness_scaled\n\nLet‚Äôs search for \\(k=3\\) similar songs to a query point \\(\\textrm{songIndex} = 6\\).\n\n# Select song and column attributes\nsongIndex = 4 # query point\ncolumns = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', \n           'loudness_scaled', 'tempo', \n           'speechiness', 'valence']\n\n# Set query parameters\nfunc, param = knnQuery,3\n\n# Implement query\nresponse = querySimilars(df, columns, songIndex, func, param)\n\nprint(\"---- Query Point ----\")\nprint(getMusicName(df.loc[songIndex]))\nprint('---- k = 3 similar songs ----')\nfor track_id in response[0]:\n    track_name = getMusicName(df.loc[track_id])\n    print(track_name)\nprint('---- k = 3 nonsimilar songs ----')\nfor track_id in response[1]:\n    track_name = getMusicName(df.loc[track_id])\n    print(track_name)\n\n---- Query Point ----\nTate McRae - greedy\n---- k = 3 similar songs ----\nBeyonc√© - TEXAS HOLD 'EM\nKali Uchis - Igual Que Un √Ångel (with Peso Pluma)\nKygo - Whatever\n---- k = 3 nonsimilar songs ----\n21 Savage - redrum\nSZA - Saturn\nBenson Boone - Slow It Down\n\n\nThe code below implements the same idea as above, but queries each track in a given playlist instead of a single defined query point.\nTo keep track of the number of songs that are similar and those that are not, we use two dictionaries: similar_count‚Äù and ‚Äúnonsimilar_count‚Äù. To do this, we create a loop that goes through the data, running the querySimilars function on each track. A loop then processes ‚Äúsimilar‚Äù and ‚Äúnon-similar‚Äù songs from the results of the query, stored in the ‚Äúresponse‚Äù variable. If a ‚Äúsimilar‚Äù song is found, its name is retrieved using the getMusicName function. The song‚Äôs name is then added to the ‚Äúsimilar_count‚Äù dictionary with a count of 1, or incremented if it already exists.\nThe same process is repeated for the ‚Äúnon-similar‚Äù songs, except the count is added to the ‚Äúnonsimilar_count‚Äù dictionary instead.\n\nsimilar_count = {} # Similar songs count\nnonsimilar_count = {} # Non-similar songs count\n\nfor track_index in df.index:\n    # Implement query\n    response = querySimilars(df, columns, track_index, func, param)\n    \n    # Get similar songs\n    for similar_index in response[0]:\n        track = getMusicName(df.loc[similar_index])\n        if track in similar_count:\n            similar_count[track] += 1\n        else:\n            similar_count[track] = 1\n    \n    # Get non-similar songs\n    for nonsimilar_index in response[1]:\n        track = getMusicName(df.loc[nonsimilar_index])\n        if track in nonsimilar_count:\n            nonsimilar_count[track] += 1\n        else:\n            nonsimilar_count[track] = 1\n\nNext, we display both the non-similar and similar songs with their respective track name and count.\n\nnonsimilar = dict(sorted(nonsimilar_count.items(), key=lambda item: item[1], reverse=True))\nprint('---- NON SIMILAR SONG COUNTS ----')\nfor track_name, track_count in nonsimilar.items():\n    if track_count &gt;= 8:\n        print(track_name, ':', track_count)\n\nsimilar = dict(sorted(similar_count.items(), key=lambda item: item[1], reverse=True))\nprint('\\n---- SIMILAR SONG COUNTS ----')\nfor track_name, track_count in similar.items():\n    if track_count &gt;= 5:\n        print(track_name, ':', track_count)\n\n---- NON SIMILAR SONG COUNTS ----\nSZA - Saturn : 35\nBenson Boone - Slow It Down : 35\n21 Savage - redrum : 34\nGood Neighbours - Home : 16\nBillie Eilish - What Was I Made For? [From The Motion Picture \"Barbie\"] : 15\nZach Bryan - I Remember Everything (feat. Kacey Musgraves) : 15\n\n---- SIMILAR SONG COUNTS ----\nDua Lipa - Training Season : 6\nMiley Cyrus - Flowers : 5\nDoja Cat - Agora Hills : 5\nSabrina Carpenter - Feather : 5\nFuture - Type Shit : 5\n\n\nAs shown above, the code snippet arranges the ‚Äúnonsimilar_count‚Äù dictionary in a descending sequence, followed by presenting the tracks with the highest non-similar query counts. We repeat the same process for songs that are similar from the ‚Äúsimilar_count‚Äù dictionary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "index.html#organized-songs-in-a-playlist",
    "href": "index.html#organized-songs-in-a-playlist",
    "title": "NLP Song Lyrics",
    "section": "Organized Songs in a Playlist",
    "text": "Organized Songs in a Playlist\nBelow, we import the Python pandas, matplotlib.pyplot, and sklearn libraries to our project. These tools help us perform various operations such as clustering, decomposition, and data visualization.\nWe then obtain a list of songs including their name and various attributes such as acousticness, danceability, energy, instrumentalness, liveness, speechiness, tempo, valence, and loudness. Next, we gather helpful insights about these songs using the‚Äô describe‚Äô function.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import cluster, decomposition\n\nsongs = df[['name','acousticness', 'danceability', 'energy', 'instrumentalness', \n            'liveness', 'speechiness', 'tempo', 'valence',  'loudness_scaled']]\nsongs.describe()\n\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nenergy\ninstrumentalness\nliveness\nspeechiness\ntempo\nvalence\nloudness_scaled\n\n\n\n\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n0.221320\n0.667160\n0.614022\n0.004662\n0.156268\n0.065064\n123.489040\n0.488798\n0.770202\n\n\nstd\n0.256806\n0.135642\n0.141416\n0.020898\n0.102940\n0.037988\n25.393903\n0.264482\n0.181740\n\n\nmin\n0.000938\n0.411000\n0.091100\n0.000000\n0.023200\n0.029800\n77.002000\n0.056900\n0.000000\n\n\n25%\n0.038000\n0.559500\n0.539750\n0.000000\n0.095525\n0.037175\n107.957000\n0.267250\n0.711771\n\n\n50%\n0.127000\n0.678000\n0.619500\n0.000003\n0.116500\n0.055000\n119.496500\n0.453500\n0.792509\n\n\n75%\n0.265750\n0.786500\n0.707250\n0.000190\n0.168000\n0.073200\n137.744250\n0.737250\n0.894701\n\n\nmax\n0.959000\n0.943000\n0.946000\n0.135000\n0.500000\n0.200000\n181.489000\n0.934000\n1.000000\n\n\n\n\n\n\n\n\nExtracting the song labels from the dataset is the first crucial step. Then, we must select the appropriate features that will serve as inputs for the Affinity Propagation clustering algorithm from the scikit-learn library. During the clustering process, a preference value of -200 is used to ensure optimal performance. Once the data is inputted, the algorithm is trained to achieve the desired outcome.\n\nlabels = songs.values[:,0]\nX = songs.values[:,1:10]\nkmeans = cluster.AffinityPropagation(preference=-200)\nkmeans.fit(X)\n\nAffinityPropagation(preference=-200)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AffinityPropagationAffinityPropagation(preference=-200)\n\n\nThe script below utilizes a dictionary called ‚Äúpredictions‚Äù to keep track of the outcomes of a comparison process between two lists: ‚Äúkmeans.predict(X)‚Äù and ‚Äúlabels‚Äù. For each new value, a unique key is generated in the dictionary with the corresponding value from the ‚Äúlabels‚Äù list appended to the key‚Äôs list of values.\nAfter sorting all values into their designated keys, we proceed to display each key alongside its relevant values. The output displays each category and the corresponding songs it contains.\n\npredictions = {}\nfor p,n in zip(kmeans.predict(X),labels):\n    if not predictions.get(p):\n        predictions[p] = []\n        \n    predictions[p] += [n]\n\nfor p in predictions:\n    print(\"Category\",p)\n    print(\"-----\")\n    for n in predictions[p]:\n        print(n)\n    print(\"\")\n\nCategory 6\n-----\nBeautiful Things\nLovin On Me\nPaint The Town Red\nWhatever She Wants\nIgual Que Un √Ångel (with Peso Pluma)\nPopular (with Playboi Carti & Madonna)\nWhatever\nLA FALDA\nIs It Over Now? (Taylor's Version) (From The Vault)\nStanding Next to You\n\nCategory 4\n-----\nobsessed\nTraining Season\nFeather\nPunter√≠a\nAgora Hills\nNever Lose Me (feat. SZA & Cardi B)\nMade For Me\nMake You Mine\nCONTIGO (with Ti√´sto)\nSeven (feat. Latto)\nDaylight\n\nCategory 0\n-----\nwe can't be friends (wait for your love)\ngreedy\nTEXAS HOLD 'EM\nStick Season\nWater\nyes, and?\nMy Love Mine All Mine\nHoudini\nToo Sweet\nAustin\nFlowers\n\nCategory 1\n-----\nLose Control\nEnd of Beginning\ni like the way you kiss me\n\nCategory 2\n-----\nSaturn\nredrum\nCruel Summer\nStrangers\nSlow It Down\n\nCategory 5\n-----\nOne Of The Girls (with JENNIE, Lily Rose Depp)\nI Remember Everything (feat. Kacey Musgraves)\nHome\nWhat Was I Made For? [From The Motion Picture \"Barbie\"]\n\nCategory 3\n-----\nScared To Start\nexes\nSnooze\nType Shit\nvampire\nRich Baby Daddy (feat. Sexyy Red & SZA)\n\n\n\nThe script successfully categorized the playlist into 6 distinct groups based on shared features, resulting in a diverse selection of songs within each category.\n\n\nK Means Clustering\nUsing K Means clustering, we choose to break the playlist into 3 smaller playlists.\nAs shown below, we employ the KMeans algorithm, obtained from the sklearn.cluster library, to cluster a collection of songs into distinct categories based on track features, such as their energy levels and sound qualities. Using three clusters, we apply this algorithm on the track features from the ‚Äúplaylist_tracks‚Äù subset of data, dropping the ‚Äúartist‚Äù and ‚Äúname‚Äù columns.\n\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\n%matplotlib inline\nplaylist_tracks = df[['artist','name','acousticness','danceability','energy',\n                      'liveness', 'instrumentalness','speechiness','valence']]\n\nkmeans = KMeans(n_clusters = 3)\nkmeans.fit(playlist_tracks.drop(['artist', 'name'], axis = 1))\n\nKMeans(n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3)\n\n\nAdditionally, we import the ‚Äúseaborn‚Äù library and use the ‚Äú%matplotlib inline‚Äù command to generate useful plots displayed inline. Below, we generate a count plot with x-axis values set to a list of groups, displayed as string representations, produced by the k-means clustering algorithm.\n\nsns.countplot(x=[str(group) for group in kmeans.labels_], color = 'lightblue')\n\n\n\n\n\n\n\n\n\n\nVisualizing the Clusters\nMoving forward, let‚Äôs look at differences in the audio features of each group.\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(playlist_tracks.drop(['artist', 'name'], axis = 1))\nscaled_data = scaler.transform(playlist_tracks.drop(['artist', 'name'], axis = 1))\n\n\n\nCode\nfrom sklearn.decomposition import PCA\npca = PCA(n_components =2)\npca.fit(scaled_data)\ndata_pca = pca.transform(scaled_data)\n\nplt.scatter(data_pca[:,0], data_pca[:,1], c = list(kmeans.labels_), cmap = 'Paired')\nplt.xlabel('PC1: {:.3f}'.format(pca.explained_variance_ratio_[0]), size = 15)\nplt.ylabel('PC2: {:.3f}'.format(pca.explained_variance_ratio_[1]), size = 15)\n\n\nText(0, 0.5, 'PC2: 0.172')\n\n\n\n\n\n\n\n\n\n\n\nText(0, 0.5, 'PC2: 0.172')\n\n\n\n\n\n\n\n\n\n\nplaylist_tracks['group'] = list(kmeans.labels_)\nplaylist_tracks = playlist_tracks.astype({'group': str})\n\nmeans = pd.DataFrame(index = range(0,3), \n                    columns = list(playlist_tracks[playlist_tracks['group'] == '0'].describe().loc['mean'].index))\nmeans.iloc[0] = playlist_tracks[playlist_tracks['group'] == '0'].describe().loc['mean']\nmeans.iloc[1] = playlist_tracks[playlist_tracks['group'] == '1'].describe().loc['mean']\nmeans.iloc[2] = playlist_tracks[playlist_tracks['group'] == '2'].describe().loc['mean']\nmeans\n\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nenergy\nliveness\ninstrumentalness\nspeechiness\nvalence\n\n\n\n\n0\n0.131965\n0.63481\n0.602714\n0.181448\n0.003759\n0.061629\n0.290567\n\n\n1\n0.73775\n0.538875\n0.452638\n0.127125\n0.017273\n0.051725\n0.333125\n\n\n2\n0.11394\n0.748381\n0.68681\n0.14219\n0.000761\n0.073581\n0.746333",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/01_data.html",
    "href": "01_sentiment_analysis/01_data.html",
    "title": "2¬† Extracting Spotify Data",
    "section": "",
    "text": "Getting started, we want to extract data for a set of tracks within one of Spotify‚Äôs top-featured playlists. Leveraging the Spotify Web API, we can seamlessly obtain detailed data for a song, such as the performing artist, the album it belongs to, its release date, popularity, and audio features like danceability, energy, and tempo.\nPython libraries like spotipy offer a user-friendly way to interact with the Spotify API, offering a range of functions that streamline tasks like API authentication, retrieving playlist data, and obtaining information about any given song.\n\n2.0.1 Accessing the Spotify Web API\nTo access data from Spotify, we import the spotipy library and the SpotifyClientCredentials module. Additionally, we utilize the pandas package for data manipulation and display. In order to authenticate our access to the Spotify API, we must provide our client ID and client secret to a client credentials manager. Once authenticated, we can use the spotipy module to interact with the Spotify API and retrieve data.\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport pandas as pd\n\nclient_id = \"xxx\"\nclient_secret = \"xxx\"\nmy_auth = SpotifyClientCredentials(client_id, client_secret)\nsp = spotipy.Spotify(auth_manager=my_auth)\n\n\n2.0.1.1 Spotify‚Äôs Featured Playlists\nLet‚Äôs take a look at the popular Spotify playlists. Below, the code retrieves a range of Spotify playlists and generates a dataframe containing details for each playlist, including its name, ID, description, thumbnail, total number of tracks, and follower count. The resulting dataframe is displayed as an HTML table.\n\nusername = \"spotify\"\nspotify_playlists = sp.user_playlists(username)\ntop_playlists = get_top_playlists(username, 6)\n\nThe function get_top_playlists retrieves all playlists for a given user and returns the playlists with the most followers. Specifically, the function gets all playlists from a given user, iterating over each playlist item to extract the thumbnail image URL, the playlist name, ID, description, total number of tracks, and follower count for the playlist. It then creates a DataFrame from the playlist data, sorts the DataFrame by the number of followers in descending order, and returns the top playlists with the most followers.\n\n\n\n\n\n\n\nthumbnail\nname\nid\ndescription\ntracks\nfollowers\n\n\n\n\n0\n\nToday‚Äôs Top Hits\n37i9dQZF1DXcBWIGoYBM5M\nOlivia Rodrigo is on top of the Hottest 50!\n50\n34686033\n\n\n1\n\nRapCaviar\n37i9dQZF1DX0XUsuxWHRQd\nNew music from Future and Metro Boomin.\n50\n15846375\n\n\n3\n\nViva Latino\n37i9dQZF1DX10zKzsJ2jva\nToday's top Latin hits, elevando nuestra m√∫sica. Shakira tiene algo especial para ti. ‚ù§Ô∏è‚Äçüî•\n50\n14838128\n\n\n7\n\nRock Classics\n37i9dQZF1DWXRqgorJj26U\nRock legends & epic songs that continue to inspire generations. Cover: Nirvana\n200\n12095843\n\n\n12\n\nAll Out 2000s\n37i9dQZF1DX4o1oenSJRJd\nThe biggest songs of the 2000s. Cover: Kelly Clarkson\n150\n11125180\n\n\n14\n\nAll Out 80s\n37i9dQZF1DX4UtSsGT1Sbe\nThe biggest songs of the 1980s. Cover: Madonna\n150\n10805624\n\n\n\n\n\n\n\n\n\n\n2.0.2 Extracting Tracks From a Playlist\nThe following script enables the compilation of song and artist data from any Spotify playlist through its URI. To analyze a particular playlist, simply copy the URI from the Spotify Player interface and input it into the function defined below. The get_playlist_tracks method returns a complete list of track IDs and corresponding artists from the selected playlist.\n\ndef get_playlist_tracks(playlist_URI):\n    tracks = []\n    results = sp.playlist_tracks(playlist_URI)\n    tracks = results[\"items\"]\n    while results[\"next\"]:\n        results = sp.next(results)\n        tracks.extend(results[\"items\"])\n    return tracks\n\n\n2.0.2.1 Extracting Features from Tracks\nThe following script utilizes Spotify‚Äôs API to extract further details about each song within the playlist. It obtains metadata such as the track name, the artist it‚Äôs sung by, the album it belongs to, the release date, and track features such as danceability, tempo, and popularity.\n\ndef playlist_features(id, artist_id, playlist_id):\n    meta = sp.track(id)\n    audio_features = sp.audio_features(id)\n    artist_info = sp.artist(artist_id)\n    playlist_info = sp.playlist(playlist_id)\n\n    # print(audio_features)\n\n    if audio_features[0] is None:\n        return None\n    \n    \n\n    name = meta['name']\n    track_id = meta['id']\n    album = meta['album']['name']\n    artist = meta['album']['artists'][0]['name']\n    artist_id = meta['album']['artists'][0]['id']\n    release_date = meta['album']['release_date']\n    length = meta['duration_ms']\n    popularity = meta['popularity']\n\n    artist_pop = artist_info[\"popularity\"]\n    artist_genres = artist_info[\"genres\"]\n\n    acousticness = audio_features[0]['acousticness']\n    danceability = audio_features[0]['danceability']\n    energy = audio_features[0]['energy']\n    instrumentalness = audio_features[0]['instrumentalness']\n    liveness = audio_features[0]['liveness']\n    loudness = audio_features[0]['loudness']\n    speechiness = audio_features[0]['speechiness']\n    tempo = audio_features[0]['tempo']\n    valence = audio_features[0]['valence']\n    key = audio_features[0]['key']\n    mode = audio_features[0]['mode']\n    time_signature = audio_features[0]['time_signature']\n    \n    playlist_name = playlist_info['name']\n\n    return [name, track_id, album, artist, artist_id, release_date, length, popularity, \n            artist_pop, artist_genres, acousticness, danceability, \n            energy, instrumentalness, liveness, loudness, speechiness, \n            tempo, valence, key, mode, time_signature, playlist_name]\n\nChoose a specific playlist to analyze by copying the URL from the Spotify Player interface. Using that link, the playlist_tracks method retrieves a list of IDs and corresponding artists for each track from the playlist. Specifically, we analyze Spotify‚Äôs Today‚Äôs Top Hits playlist.\n\nplaylist_links = [top_playlists['id'][0]]\n\nfor playlist_URI in playlist_links:\n    # playlist_URI = link.split(\"/\")[-1].split(\"?\")[0]\n    \n    all_tracks = [  # Loop over track ids\n    playlist_features(i[\"track\"][\"id\"], i[\"track\"][\"artists\"][0][\"uri\"], playlist_URI)\n    for i in get_playlist_tracks(playlist_URI)\n]\n\nPutting it all together, the get_playlist_tracks function retrieves basic details for each song in a specified Spotify playlist using its URI. The playlist_features function then iterates through these tracks using their IDs to extract additional information, such as danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo, and more. From there, we create a Pandas dataframe by passing in the extracted information.\n\n# Create dataframe\ndf = pd.DataFrame(\n    all_tracks, columns=['name', 'track_id', 'album', 'artist', 'artist_id','release_date',\n                     'length', 'popularity', 'artist_pop', 'artist_genres',\n                     'acousticness', 'danceability', 'energy',\n                     'instrumentalness', 'liveness', 'loudness',\n                     'speechiness', 'tempo', 'valence', 'key', 'mode',\n                     'time_signature', 'playlist'])\n\ndf.tail(4)\n\n\n\n\n\n\n\n\n\nname\ntrack_id\nalbum\nartist\nartist_id\nrelease_date\nlength\npopularity\nartist_pop\nartist_genres\n...\ninstrumentalness\nliveness\nloudness\nspeechiness\ntempo\nvalence\nkey\nmode\ntime_signature\nplaylist\n\n\n\n\n46\nDaylight\n1odExI7RdWc4BT515LTAwj\nDaylight\nDavid Kushner\n33NVpKoXjItPwUJTMZIOiY\n2023-04-14\n212953\n91\n77\n[gen z singer-songwriter, singer-songwriter pop]\n...\n0.000441\n0.093\n-9.475\n0.0335\n130.090\n0.324\n2\n0\n4\nToday‚Äôs Top Hits\n\n\n47\nLA FALDA\n7iUtQNMRB8ZkKC4AmEuCJC\nLVEU: VIVE LA TUYA...NO LA MIA\nMyke Towers\n7iK8PXO48WeuP03g8YR51W\n2023-09-18\n174229\n92\n87\n[reggaeton, trap latino, urbano latino]\n...\n0.008770\n0.190\n-4.260\n0.0691\n103.008\n0.267\n7\n1\n4\nToday‚Äôs Top Hits\n\n\n48\nIs It Over Now? (Taylor's Version) (From The V...\n1Iq8oo9XkmmvCQiGOfORiz\n1989 (Taylor's Version)\nTaylor Swift\n06HL4z0CvFAxyc27GXpf02\n2023-10-26\n229477\n89\n100\n[pop]\n...\n0.000000\n0.127\n-7.346\n0.0360\n100.012\n0.176\n0\n1\n4\nToday‚Äôs Top Hits\n\n\n49\nStanding Next to You\n2KslE17cAJNHTsI2MI0jb2\nGOLDEN\nJung Kook\n6HaGTQPmzraVmaVxvz6EUc\n2023-11-03\n206019\n92\n86\n[k-pop]\n...\n0.000000\n0.339\n-4.389\n0.0955\n106.017\n0.816\n2\n0\n4\nToday‚Äôs Top Hits\n\n\n\n\n4 rows √ó 23 columns",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/02_lyrics.html",
    "href": "01_sentiment_analysis/02_lyrics.html",
    "title": "3¬† Natural Language Processing",
    "section": "",
    "text": "3.0.1 Scraping the Web\nTo get started, the script below imports lyricsgenius, a fundamental package libary allowing for web scraping of the Genius Lyrics website to retrieve the lyrics of any given song. Through the initialization of the genius variable, one can access the Genius API and retrieve the lyrics of any given song, such as ‚ÄúToo Many Nights‚Äù by Metro Boomin.\nimport lyricsgenius\n\ngenius = lyricsgenius.Genius(\"epFCxujgBe-Y6WrkZedI8kerKxiCpR6Rh0DAHYNlKDf9B4H1nXTdZIkj7krNUHVV\")\nsong = genius.search_song(\"Too Many Nights\", \"Metro Boomin\")\n\nSearching for \"Too Many Nights\" by Metro Boomin...\nDone.\nFirst, we define a function that retrieves the lyrics for any song and artist from the Genius database. As shown below, it first searches for the track using the provided name and artist and then extracts the lyrics from the search results.\ndef get_song_lyrics(song_name, song_artist):\n    song_genius = genius.search_song(song_name, song_artist)\n    song_lyrics = song_genius.lyrics.partition(\"Lyrics\")[2]\n    # Remove any numbers followed by 'Embed'\n    song_lyrics = re.sub(r\"[\\[].*?[\\]]|\\d+Embed\", \"\", song_lyrics)\n    # Remove text between square brackets\n    song_lyrics = re.sub(r\"(\\-[A-Za-z]+\\-)\", \"\", song_lyrics)\n\n    return song_lyrics",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/02_lyrics.html#sentiment-analysis",
    "href": "01_sentiment_analysis/02_lyrics.html#sentiment-analysis",
    "title": "3¬† Natural Language Processing",
    "section": "3.1 Sentiment Analysis",
    "text": "3.1 Sentiment Analysis\nSubsequently, the process involves the implementation of pipeline classes to carry out predictions using models accessible in the Hub. The code imports and employs multiple transformer models specifically designed for text classification and sentiment analysis. Specifically, the following procedure creates three distinct pipelines, each equipped with different models that facilitate the assessment of emotions and sentiment in textual content.\n\nimport transformers\nfrom transformers import pipeline\n\n# Initialize Genius API and sentiment classifiers\nclassifiers = [\n    pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True),\n    pipeline(\"text-classification\", model='cardiffnlp/twitter-roberta-base-sentiment', return_all_scores=True),\n    pipeline(\"sentiment-analysis\", return_all_scores=True)\n]\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\nThe get_lyric_sentiment function takes in pre-processed lyrics as input and produces a dictionary of sentiment scores. It leverages three distinct classifiers to calculate the scores and aggregates them into a final result. For instance, one of these classifiers is the distilbert-base-uncased-emotion model, specifically trained to detect ‚Äúemotions in texts such as sadness, joy, love, anger, fear, and surprise‚Äù.\n\n# Function to perform sentiment analysis\ndef get_lyric_sentiment(lyrics, classifiers):\n    text = ' '.join(lyrics)\n    scores = {}\n    for classifier in classifiers:\n        try:\n            predictions = classifier(text, truncation=True)\n            for prediction in predictions[0]:\n                scores[prediction['label']] = prediction['score']\n        except Exception as e:\n            print(f\"Error during sentiment analysis: {e}\")\n    return scores\n\nIf the lyric sequence contains more than 512 tokens, it will trigger an error message indicating an exception encountered in the ‚Äòembeddings‚Äô layer. However, we have implemented measures to properly manage lyric sequences that exceed 512 words in the function mentioned above.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/02_lyrics.html#putting-it-all-together",
    "href": "01_sentiment_analysis/02_lyrics.html#putting-it-all-together",
    "title": "3¬† Natural Language Processing",
    "section": "3.2 Putting it All Together",
    "text": "3.2 Putting it All Together\nTo summarize, the code efficiently collects data and performs text analysis on every song in a playlist. Specifically, it systematically processes a list of tracks and corresponding artists while simultaneously conducting a thorough cleaning procedure on the lyrics. The cleaning process involves removing all nonessential characters, resulting in a more precise depiction of the song‚Äôs content. The outcome is a comprehensive frequency analysis of each word in a song‚Äôs lyrics, providing deeper insights into the overall conveyed message.\nAdditionally, the program computes a sentiment score for each song based on the lyrics, indicating whether the lyrics are positive, negative, or neutral. It also collects information about the song and artist, such as the release date, length, popularity, and genre. Finally, the program compiles all this information into a dataframe for further analysis.\n\ntrack_data = []\nfor i, track in all_tracks.iterrows():\n\n    song_name = track[\"name\"] #.partition(\" (\")[0]\n    song_name = track['name'].partition(\" (with\")[0]\n    song_name = song_name.partition(\" - From\")[0]\n\n    artist_name = track[\"artist\"]\n\n    try:\n        track_lyrics = clean_song_lyrics(song_name, artist_name)\n        stopwords_removed = remove_stopwords_lyrics(track_lyrics)\n        lemmatized = word_lemmatize(stopwords_removed)\n\n        sentiment_scores = get_lyric_sentiment(stopwords_removed, classifiers)\n\n        track_info = track.to_dict()\n        track_info.update(sentiment_scores)\n\n        track_info[\"lyrics\"] = track_lyrics\n        track_info[\"stopwords_removed\"] = stopwords_removed\n        track_info[\"lemmatized\"] = lemmatized\n\n        track_data.append(track_info)\n\n    except Exception as e:\n        print(f\"Error processing track {track['name']} by {track['artist']}: {e}\")\n\ndf_tracks = pd.DataFrame(track_data)\n\n\n#df_tracks = pd.DataFrame(track_data)\ndf_tracks.to_csv(\"../assets/data/all_tracks+lyrics.csv\", index=False)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Natural Language Processing</span>"
    ]
  }
]