[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP Song Lyrics",
    "section": "",
    "text": "Overview\nObjective. The following project involves extracting and analyzing Spotify data from top playlists using the Spotify and Genius Lyrics Web API. Natural Language Processing techniques are used to process lyrics and perform sentiment analysis. K-means clustering and PCA analysis are employed to categorize songs and analyze relationships between musical features.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#extracting-spotify-data",
    "href": "index.html#extracting-spotify-data",
    "title": "NLP Song Lyrics",
    "section": "Extracting Spotify Data",
    "text": "Extracting Spotify Data\nGetting started, we want to extract data for a set of tracks within one of Spotify’s top-featured playlists. Leveraging the Spotify Web API, we can seamlessly obtain detailed data for a song, such as the artist, the album it belongs to, its release date, popularity, and audio features like danceability, energy, and tempo.\n\nAccessing the Spotify Web API\nPython libraries like spotipy offer a user-friendly way to interact with the Spotify API, offering a range of functions that streamline tasks like API authentication, retrieving playlist data, and obtaining information for any given song. To authenticate access, we provide our client ID and secret. Once authenticated, we can interact with the API and retrieve data.\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\n# Set client id and client secret\nmy_auth = SpotifyClientCredentials(client_id = \"xxx\", client_secret = \"xxx\")\nsp = spotipy.Spotify(auth_manager=my_auth)  # Spotify authentication\n\n\nSpotify’s Featured Playlists\nLet’s take a look at the popular Spotify playlists. Below, the code retrieves a range of Spotify playlists and generates a dataframe containing details for each playlist.\n\n\n\n\n\n\n\nthumbnail\nplaylist_name\nplaylist_id\ndescription\ntotal\n\n\n\n\n0\n\nToday’s Top Hits\n37i9dQZF1DXcBWIGoYBM5M\nDua Lipa is on top of the Hottest 50!\n50\n\n\n1\n\nRapCaviar\n37i9dQZF1DX0XUsuxWHRQd\nNew music from Future and Metro Boomin.\n50\n\n\n2\n\nHot Country\n37i9dQZF1DX1lVhptIYRda\nToday's top country hits. Cover: Tyler Hubbard\n50\n\n\n\n\n\n\n\n\n\n\nExtracting Track Data From Playlist\nNext, we utilize Spotify’s API to extract further details about each song within the playlist. We obtain metadata such as the track name, the artist it’s sung by, the album it belongs to, the release date, and track features such as danceability, tempo, and popularity.\n\ndef get_playlist_tracks(playlist_URI):\n    results = sp.playlist_tracks(playlist_URI)\n    tracks = results[\"items\"]\n    while results[\"next\"]:\n        results = sp.next(results)\n        tracks.extend(results[\"items\"])\n    return tracks\n\nChoose a specific playlist to analyze by copying the URL from the Spotify Player interface. Using that link, the playlist_tracks method retrieves a list of IDs and corresponding artists for each track from the playlist. Specifically, we analyze Spotify’s Today’s Top Hits playlist.\n\n\n\n\n\n\n\n\n\n\nname\nartist\npopularity\nartist_genres\nrelease_date\n\n\n\n\n0\nBeautiful Things\nBenson Boone\n100\n['singer-songwriter pop']\n2024-01-18\n\n\n1\nOKLOSER\nDoja Cat\n69\n['dance pop', 'pop']\n2024-04-05\n\n\n2\ni like the way you kiss me\nArtemas\n95\n[]\n2024-03-19",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#natural-language-processing",
    "href": "index.html#natural-language-processing",
    "title": "NLP Song Lyrics",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nUsing the data gathered from the Spotify API, we now want to extract and process the lyrics for each song. This is accomplished through scraping textual lyrical data from the Genius Lyrics website. Following extraction, the lyrics are thoroughly processed and cleaned before undergoing sentiment analysis.\n\nScraping the Web\nThe lyricsgenius is a fundamental library allowing for web scraping of the Genius Lyrics website. Through the initialization of the genius variable, one can access the Genius API and retrieve the lyrics of any given song, such as “Too Many Nights” by Metro Boomin.\n\nimport lyricsgenius\ngenius = lyricsgenius.Genius(access_token)\nsong = genius.search_song(\"Too Many Nights\", \"Metro Boomin\")\n\nSearching for \"Too Many Nights\" by Metro Boomin...\nDone.\n\n\n\n\nPre-Processing Text Data\nUsing the genius library, we define a function to fetch the lyrics of a song given the name and artist. Once retrieved, the next step is to pre-process the lyrics. This involves a cleaning process to eliminate patterns that may hinder the overall readability. The script contains the following steps:\n\n\nFetching Lyrics\nExpanding Contractions\nConverting Text to Lowercase\nSpell Checking + Censoring\nRemoving Punctuations\nTokenization\n\n\n\ndef clean_song_lyrics(song_name, artist_name):\n    # Fetch song lyrics and clean\n    lyrics = get_song_lyrics(song_name, artist_name) \n    lyrics = profanity.censor(contractions.fix(lyrics).lower(), censor_char=\"\")\n    lyrics = remove_punctuation(lyrics) \n    # Tokenizing and encoding to ASCII\n    return [word.encode(\"ascii\", \"ignore\").decode() for word in word_tokenize(lyrics)]\n\nOverall, the clean_song_lyrics function extracts lyrics from Genius database, expands contractions, removes repetitive phrases, corrects spelling, and eliminates profanity. It returns cleaned, tokenized, and encoded lyrics as a list of words.\n\nFurther Text Cleaning\nWe employ the Natural Language Toolkit (NLTK) library and its WordNetLemmatizer tool to filter out stopwords and perform lemmatization. Removing common words like “the” or “and” helps condense the text, allowing for a more thorough analysis of the lyrics’ core message. Lemmatization helps standardize words by transforming different variations of the same verb into their most basic form. See the full documentation here.\n\n\n\n\n\n\n\n\n\n\nname\nartist\nlyrics\nstopwords_removed\nlemmatized\n\n\n\n\n0\nBeautiful Things\nBenson Boone\n['for', 'a', 'while', 'there', 'it', 'was', 'rough'...\n['rough', 'lately', 'better', 'last', 'four', 'cold...\n['rough', 'lately', 'well', 'last', 'four', 'cold',...\n\n\n1\nOKLOSER\nDoja Cat\n['i', 'say', 'okay', 'okay', 'loser', 'okay', 'lose...\n['say', 'okay', 'okay', 'loser', 'okay', 'loser', '...\n['say', 'okay', 'okay', 'loser', 'okay', 'loser', '...\n\n\n2\ni like the way you kiss me\nArtemas\n['i', 'like', 'the', 'way', 'you', 'kiss', 'me', 'i...\n['like', 'way', 'kiss', 'like', 'way', 'uh', 'like'...\n['like', 'way', 'kiss', 'like', 'way', 'uh', 'like'...\n\n\n\n\n\n\n\n\n\n\nTerm Frequency Analysis\n\nLet’s examine the most frequent words. Plotting the frequency distribution helps to determine the occurrence of the most common terms in our lyrical corpus.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#sentiment-analysis",
    "href": "index.html#sentiment-analysis",
    "title": "NLP Song Lyrics",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nSubsequently, the process involves the implementation of pipeline classes to carry out predictions using models accessible in the Hub. The code imports and employs multiple transformer models specifically designed for text classification and sentiment analysis. Specifically, the following procedure creates three distinct pipelines, each equipped with different models that facilitate the assessment of emotions and sentiment in textual content.\n\nimport transformers\nfrom transformers import pipeline\n\n# Initialize Genius API and sentiment classifiers\nclassifiers = [\n    pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True),\n    pipeline(\"text-classification\", model='cardiffnlp/twitter-roberta-base-sentiment', return_all_scores=True),\n    pipeline(\"sentiment-analysis\", return_all_scores=True)\n]\n\nThe get_lyric_sentiment function takes in pre-processed lyrics as input and produces a dictionary of sentiment scores. It leverages three distinct classifiers to calculate the scores and aggregates them into a final result. For instance, one of these classifiers is the distilbert-base-uncased-emotion model, specifically trained to detect “emotions in texts such as sadness, joy, love, anger, fear, and surprise”.\n\n# Function to perform sentiment analysis\ndef get_lyric_sentiment(lyrics, classifiers):\n    text = \" \".join(lyrics)\n    scores = {}\n    for classifier in classifiers:\n        try:\n            predictions = classifier(text, truncation=True)\n            for prediction in predictions[0]:\n                scores[prediction[\"label\"]] = prediction[\"score\"]\n        except Exception as e:\n            print(f\"Error during sentiment analysis: {e}\")\n    return scores",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#putting-it-all-together",
    "href": "index.html#putting-it-all-together",
    "title": "NLP Song Lyrics",
    "section": "Putting it All Together",
    "text": "Putting it All Together\nTo summarize, the code efficiently collects data and performs text analysis on every song in a playlist. Specifically, it systematically processes a list of tracks and corresponding artists while simultaneously conducting a thorough cleaning procedure on the lyrics. Additionally, the program computes a sentiment score for each song based on the lyrics, indicating whether the lyrics are positive, negative, or neutral.\n\n\n\n\n\n\n\n\n\n\nname\nalbum\nartist\nrelease_date\nlength\npopularity\nartist_pop\nartist_genres\nacousticness\ndanceability\n...\njoy\nlove\nanger\nfear\nsurprise\nLABEL_0\nLABEL_1\nLABEL_2\nNEGATIVE\nPOSITIVE\n\n\n\n\n0\nBeautiful Things\nBeautiful Things\nBenson Boone\n2024-01-18\n180304\n100\n86\n['singer-songwriter pop']\n0.151000\n0.472\n...\n0.000454\n0.000210\n0.000819\n0.996218\n0.000817\n0.033695\n0.346160\n0.620145\n0.012751\n0.987248\n\n\n1\nOKLOSER\nScarlet 2 CLAUDE\nDoja Cat\n2024-04-05\n169066\n69\n87\n['dance pop', 'pop']\n0.025500\n0.898\n...\n0.049557\n0.021035\n0.072808\n0.021095\n0.002085\n0.366062\n0.583096\n0.050843\n0.996938\n0.003062\n\n\n2\ni like the way you kiss me\ni like the way you kiss me\nArtemas\n2024-03-19\n142514\n95\n81\n[]\n0.000938\n0.599\n...\n0.885468\n0.089855\n0.003870\n0.001377\n0.001145\n0.034661\n0.521730\n0.443609\n0.984495\n0.015505\n\n\n\n\n3 rows × 33 columns\n\n\n\n\nIn summary, the above code aims to collect and refine song lyrics by eliminating stopwords and conducting lemmatization. Subsequently, it employs pre-trained models for sentiment analysis to determine the prevailing emotion conveyed in the lyrics. Finally, the program compiles all this information into a dataframe for further analysis.\n\n\nTwitter-roBERTa-base for Sentiment Analysis\nNow, we present a graphical representation of the results obtained from the roBERTa-base model “trained on roughly 58 million tweets and fine-tuned for sentiment analysis using the TweetEval benchmark” (EMNLP 2020). According to the TweetEval reference paper and official Github repository, the resulting labels 0, 1, and 2 correspond to Negative, Neutral, and Positive, respectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#correlations-matrix",
    "href": "index.html#correlations-matrix",
    "title": "NLP Song Lyrics",
    "section": "Correlations Matrix",
    "text": "Correlations Matrix\nAfter completing the initial data analysis, we proceed with generating the Pearson correlations matrix using the Pandas command df.corr(). Subsequently, we visualize the matrix using the seaborn heatmap, providing a detailed understanding of the relationships between the various variables in our dataset.\n\ntrack_sentiment_df = df_final[['name', 'artist',\n           'acousticness', 'danceability', 'energy', 'instrumentalness', \n           'loudness', 'speechiness', 'tempo', 'valence', \n           'sadness', 'joy', 'love', 'anger', 'fear', 'surprise',\n           'LABEL_0', 'LABEL_1', 'LABEL_2', 'NEGATIVE', 'POSITIVE']]\n\n# Find the pearson correlations matrix\ncorr = track_sentiment_df.corr(method = 'pearson')\n\n\n\n\n\n\n\n\n\n\nThe code below produces a scatterplot that showcases the correlation between energy and fear. The x-axis represents the energy value, while the y-axis represents the fear sentiment. The size of each data point corresponds to the label indicating the neutral sentiment level, and its color represents the valence value. Moreover, each bubble contains its energy value within, allowing for a straightforward interpretation of the data.\n\n\nText(0.5, 1.0, 'Fear vs. Energy')\n\n\n\n\n\n\n\n\n\nSimilarly, the scatterplot presented above utilizes the track sentiment data, comparing the energy and fear levels of the tracks based on valence and size.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#pca-analysis",
    "href": "index.html#pca-analysis",
    "title": "NLP Song Lyrics",
    "section": "PCA Analysis",
    "text": "PCA Analysis\nNext, we implement principal component analysis (PCA) on a comprehensive dataset comprising a range of musical features.\nFirst, we create a table from the df_final dataframe by extracting specific columns that facilitate our analysis. These columns consist of acousticness, danceability, energy, speechiness, tempo, and valence of each track, as well as emotional features such as sadness, joy, love, anger, fear, and surprise. In addition, the table includes the track name and flags for both negative and positive sentiments.\nWe then perform PCA on the data in the table and apply it to generate a biplot depicting the relationship between the features and tracks. This biplot quickly reveals any discernible patterns and clusters within the dataset.\n\nX_SMALL = df_final[['acousticness', 'danceability', 'energy', 'speechiness', \n                    'tempo', 'valence', 'sadness', 'joy', 'love', 'anger', \n                    'fear', 'surprise', 'name', 'NEGATIVE', 'POSITIVE']]\n\nTo process the data, the code employs the PCA and StandardScaler modules from the sklearn decomposition and preprocessing libraries. Specifically, the \\(X_i\\) variable is used to choose the first 12 columns from the subset of data mentioned above, while the track_name column is chosen as the target variable. Next, the StandardScaler standardizes the \\(X_i\\) data.\nPCA is applied to the standardized data, \\(X_{st}\\), using the PCA module, and the resulting loadings and eigenvalues are saved.\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom bioinfokit.visuz import cluster\n\nX_i = X_SMALL.iloc[:,0:12]\ntarget = X_SMALL['name'].to_numpy()\nX_st =  StandardScaler().fit_transform(X_i)\npca_out = PCA().fit(X_st)\n\n# component loadings\nloadings = pca_out.components_\n\n# get eigenvalues (variance explained by each PC)  \npca_out.explained_variance_\n\narray([2.60742957e+00, 1.96582173e+00, 1.51936757e+00, 1.29285435e+00,\n       1.20409389e+00, 1.12867615e+00, 9.19589146e-01, 5.12748272e-01,\n       4.40843225e-01, 3.78002524e-01, 2.75471537e-01, 9.61390304e-15])\n\n\nNext, the following code uses the PCA() function to calculate the PCA scores of the standardized data set, \\(X_{st}\\).\nA biplot is generated using the cluster module from the bioinfokit library. The biplot is based on the PCA scores and loadings, and the column names of the \\(X_i\\) data frame are used as labels for the plot. The variance explained by the first two principal components are also displayed on the plot.\n\n# get biplot\npca_scores = PCA().fit_transform(X_st)\ncluster.biplot(cscore=pca_scores, loadings=loadings, labels=X_i.columns.values, \n               var1=round(pca_out.explained_variance_ratio_[0]*100, 2),\n               var2=round(pca_out.explained_variance_ratio_[1]*100, 2), #colorlist=target,\n               show=True,dim=(10,5),dotsize=16)\n\n\n\n\n\n\n\n\nThen, I assigned the resulting column names to the variable cols_pca using a list comprehension. Using the PCA scores, column names, and the original index from \\(X_i\\), I created a new pandas DataFrame called df_pca. The first three rows of this new DataFrame is shown below.\n\npca_scores = PCA().fit_transform(X_st)\ncols_pca = [f'PC{i}' for i in range(1, pca_out.n_components_+1)]\ndf_pca = pd.DataFrame(pca_scores, columns=cols_pca, index=X_i.index)\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\n\n\n\n\n0\n2.104921\n-0.703870\n-0.449428\n0.054701\n-2.316441\n1.502806\n0.124803\n-1.210642\n1.100870\n-0.910310\n-0.629496\n-1.447422e-07\n\n\n1\n-3.723234\n-2.537185\n1.052730\n-1.604456\n-0.505719\n-1.728136\n2.323861\n-0.279028\n-0.692971\n-1.101964\n-0.180153\n-1.370107e-07\n\n\n2\n-2.202076\n0.482354\n-0.424752\n1.341098\n-0.050790\n0.763951\n-1.798994\n0.433982\n-0.490282\n-0.337022\n0.359789\n-2.117456e-09\n\n\n\n\n\n\n\n\nThe variance ratios for the PCA output and the cumulative sum of the explained variance ratios are printed below. Specifically, the array displayed represents the amount of variability explained by each component.\n\nprint(pca_out.explained_variance_ratio_)\nprint('----')\nprint(pca_out.explained_variance_ratio_.cumsum())\n\n[2.12940081e-01 1.60542108e-01 1.24081685e-01 1.05583105e-01\n 9.83343344e-02 9.21752190e-02 7.50997802e-02 4.18744422e-02\n 3.60021967e-02 3.08702062e-02 2.24968422e-02 7.85135415e-16]\n----\n[0.21294008 0.37348219 0.49756387 0.60314698 0.70148131 0.79365653\n 0.86875631 0.91063075 0.94663295 0.97750316 1.         1.        ]\n\n\nThe loading vectors help visualize the relationship between the original variables and their respective components. These vectors represent the weights of the variables within a mathematical equation used to generate the principal components.\n\ndf_weights = pd.DataFrame(pca_out.components_.T, columns=df_pca.columns, index=X_i.columns)\ndf_weights\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\n\n\n\n\nacousticness\n0.429982\n-0.185038\n-0.289349\n-0.208272\n0.255146\n0.033623\n0.102703\n0.040766\n-0.484959\n-0.037511\n0.582017\n-2.508072e-08\n\n\ndanceability\n-0.403254\n0.180085\n-0.003241\n-0.314433\n-0.008825\n0.046065\n0.516043\n0.145835\n0.251713\n0.456560\n0.380134\n-2.223341e-09\n\n\nenergy\n-0.422629\n0.047423\n0.297395\n0.281021\n0.077720\n0.239748\n-0.310296\n0.086865\n0.028693\n-0.390775\n0.575189\n-3.541813e-08\n\n\nspeechiness\n-0.322454\n-0.236476\n0.078486\n-0.003877\n-0.201256\n-0.411806\n0.503272\n-0.291856\n-0.294055\n-0.445023\n-0.029375\n4.255371e-09\n\n\ntempo\n-0.161170\n-0.456735\n0.128464\n0.397063\n-0.253591\n-0.134998\n-0.133000\n0.210921\n-0.357470\n0.562721\n0.045888\n1.989463e-08\n\n\nvalence\n-0.390247\n-0.097607\n-0.176422\n-0.325334\n0.117838\n0.402580\n-0.021178\n0.452517\n-0.385817\n-0.160074\n-0.381521\n1.303994e-08\n\n\nsadness\n-0.047228\n-0.464075\n0.209289\n-0.438591\n0.222355\n-0.317843\n-0.264417\n0.086517\n0.277165\n-0.025458\n0.025237\n4.901707e-01\n\n\njoy\n-0.207226\n0.340111\n-0.570731\n0.272893\n0.014588\n-0.259897\n-0.098486\n0.053411\n-0.077909\n0.003207\n0.032686\n5.947408e-01\n\n\nlove\n-0.004763\n0.432352\n0.455254\n-0.297329\n-0.122165\n0.063084\n-0.171260\n-0.336317\n-0.479221\n0.198448\n-0.021921\n2.941618e-01\n\n\nanger\n0.122984\n-0.060202\n0.307003\n0.399170\n0.545581\n0.255976\n0.446737\n0.001680\n-0.027313\n0.043151\n-0.167417\n3.664253e-01\n\n\nfear\n0.234053\n-0.191067\n-0.027640\n-0.014592\n-0.651966\n0.466372\n0.171745\n0.044005\n0.143826\n-0.144700\n0.083373\n4.303071e-01\n\n\nsurprise\n0.276657\n0.314524\n0.319731\n0.021877\n-0.150950\n-0.365804\n0.132030\n0.713932\n-0.059582\n-0.185139\n0.014813\n7.292557e-03",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#spotify-songs---similarity-search",
    "href": "index.html#spotify-songs---similarity-search",
    "title": "NLP Song Lyrics",
    "section": "Spotify Songs - Similarity Search",
    "text": "Spotify Songs - Similarity Search\nBelow, we create a query to retrieve similar elements based on Euclidean distance. In mathematics, the Euclidean distance between two points is the length of the line segment between the two points. In this sense, the closer the distance is to 0, the more similar the songs are.\n\nKNN Algorithm\nTo obtain a string search for a song, utilize the getMusicName function shown below, which returns the artist and song name.\nThe k-Nearest Neighbors (KNN) algorithm searches for k similar elements based on a query point at the center within a predefined radius. We execute the KNN algorithm using the knnQuery function defined below, which takes a query point, a set of characteristic points, and a value for k. It computes the sum of squared differences between each data and query point, followed by the calculation of the Euclidean distance between them. The function then arranges the points by distance and returns the k closest and farthest points.\nThe querySimilars function then removes the query point and executes the KNN algorithm on the remaining points, returning the k most similar points to the query point based on the specified columns, function, and parameter.\n\n# Get a song string search\ndef getMusicName(elem):\n    return f\"{elem['artist']} - {elem['name']}\"\n\ndef knnQuery(queryPoint, arrCharactPoints, k):\n    queryVals = queryPoint.tolist()\n    distVals = []\n    \n    # Copy of dataframe indices and data\n    tmp = arrCharactPoints.copy(deep = True)  \n    for index, row in tmp.iterrows():\n        feat = row.values.tolist()\n        \n        # Calculate sum of squared differences\n        ssd = sum(abs(feat[i] - queryVals[i]) ** 2 for i in range(len(queryVals)))\n        \n        # Get euclidean distance\n        distVals.append(ssd ** 0.5)\n        \n    tmp['distance'] = distVals\n    tmp = tmp.sort_values('distance')\n    \n    # K closest and furthest points\n    return tmp.head(k).index, tmp.tail(k).index\n\n\n# Execute KNN removing the query point\ndef querySimilars(df, columns, idx, func, param):\n    arr = df[columns].copy(deep = True)\n    queryPoint = arr.loc[idx]\n    arr = arr.drop([idx])\n    return func(queryPoint, arr, param)\n\nKNN Query Example.\nWe now establish a function that creates customized query points and alters the data columns, allowing for further exploration of various options. To illustrate, the code snippet below chooses a particular group of song features and then seeks out the top k values within that feature set that are equal to one.\nTo begin, we create a scaler utilizing the preprocessing library from sklearn. It’s worth noting that all the feature values fall within the range of 0 and 1, except for loudness. As a result, we need to scale loudness to conform to the same range.\n\ndf = df_final\n\nLet’s search for \\(k=3\\) similar songs to a query point \\(\\textrm{songIndex} = 6\\).\n\n# Select song and column attributes\nsongIndex = 4 # query point\ncolumns = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', \n           'loudness_scaled', 'tempo', \n           'speechiness', 'valence']\n\n# Set query parameters\nfunc, param = knnQuery,3\n\n# Implement query\nresponse = querySimilars(df, columns, songIndex, func, param)\n\nprint(\"---- Query Point ----\")\nprint(getMusicName(df.loc[songIndex]))\nprint('---- k = 3 similar songs ----')\nfor track_id in response[0]:\n    track_name = getMusicName(df.loc[track_id])\n    print(track_name)\nprint('---- k = 3 nonsimilar songs ----')\nfor track_id in response[1]:\n    track_name = getMusicName(df.loc[track_id])\n    print(track_name)\n\n---- Query Point ----\nAriana Grande - we can't be friends (wait for your love)\n---- k = 3 similar songs ----\nDasha - Austin\nDua Lipa - Houdini\nHozier - Too Sweet\n---- k = 3 nonsimilar songs ----\nImagine Dragons - Eyes Closed\nSZA - Saturn\nBenson Boone - Slow It Down\n\n\nThe code below implements the same idea as above, but queries each track in a given playlist instead of a single defined query point.\nTo keep track of the number of songs that are similar and those that are not, we use two dictionaries: similar_count” and “nonsimilar_count”. To do this, we create a loop that goes through the data, running the querySimilars function on each track. A loop then processes “similar” and “non-similar” songs from the results of the query, stored in the “response” variable. If a “similar” song is found, its name is retrieved using the getMusicName function. The song’s name is then added to the “similar_count” dictionary with a count of 1, or incremented if it already exists.\nThe same process is repeated for the “non-similar” songs, except the count is added to the “nonsimilar_count” dictionary instead.\n\nsimilar_count = {} # Similar songs count\nnonsimilar_count = {} # Non-similar songs count\n\nfor track_index in df.index:\n    # Implement query\n    response = querySimilars(df, columns, track_index, func, param)\n    \n    # Get similar songs\n    for similar_index in response[0]:\n        track = getMusicName(df.loc[similar_index])\n        if track in similar_count:\n            similar_count[track] += 1\n        else:\n            similar_count[track] = 1\n    \n    # Get non-similar songs\n    for nonsimilar_index in response[1]:\n        track = getMusicName(df.loc[nonsimilar_index])\n        if track in nonsimilar_count:\n            nonsimilar_count[track] += 1\n        else:\n            nonsimilar_count[track] = 1\n\nNext, we display both the non-similar and similar songs with their respective track name and count.\n\nnonsimilar = dict(sorted(nonsimilar_count.items(), key=lambda item: item[1], reverse=True))\nprint('---- NON SIMILAR SONG COUNTS ----')\nfor track_name, track_count in nonsimilar.items():\n    if track_count &gt;= 8:\n        print(track_name, ':', track_count)\n\nsimilar = dict(sorted(similar_count.items(), key=lambda item: item[1], reverse=True))\nprint('\\n---- SIMILAR SONG COUNTS ----')\nfor track_name, track_count in similar.items():\n    if track_count &gt;= 5:\n        print(track_name, ':', track_count)\n\n---- NON SIMILAR SONG COUNTS ----\nImagine Dragons - Eyes Closed : 29\nSZA - Saturn : 29\nBenson Boone - Slow It Down : 29\nBillie Eilish - What Was I Made For? [From The Motion Picture \"Barbie\"] : 21\nZach Bryan - I Remember Everything (feat. Kacey Musgraves) : 21\nGood Neighbours - Home : 21\n\n---- SIMILAR SONG COUNTS ----\nKygo - Whatever : 5\nDua Lipa - Houdini : 5\nImagine Dragons - Eyes Closed : 5\n21 Savage - redrum : 5\nDoja Cat - Agora Hills : 5\nDua Lipa - Training Season : 5\nSZA - Snooze : 5\nDrake - Rich Baby Daddy (feat. Sexyy Red & SZA) : 5\nThe Weeknd - One Of The Girls (with JENNIE, Lily Rose Depp) : 5\n\n\nAs shown above, the code snippet arranges the “nonsimilar_count” dictionary in a descending sequence, followed by presenting the tracks with the highest non-similar query counts. We repeat the same process for songs that are similar from the “similar_count” dictionary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#organized-songs-in-a-playlist",
    "href": "index.html#organized-songs-in-a-playlist",
    "title": "NLP Song Lyrics",
    "section": "Organized Songs in a Playlist",
    "text": "Organized Songs in a Playlist\nBelow, we import the Python pandas, matplotlib.pyplot, and sklearn libraries to our project. These tools help us perform various operations such as clustering, decomposition, and data visualization.\nWe then obtain a list of songs including their name and various attributes such as acousticness, danceability, energy, instrumentalness, liveness, speechiness, tempo, valence, and loudness. Next, we gather helpful insights about these songs using the’ describe’ function.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import cluster, decomposition\n\nsongs = df[['name','acousticness', 'danceability', 'energy', 'instrumentalness', \n            'liveness', 'speechiness', 'tempo', 'valence',  'loudness_scaled']]\nsongs.describe()\n\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nenergy\ninstrumentalness\nliveness\nspeechiness\ntempo\nvalence\nloudness_scaled\n\n\n\n\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n\n\nmean\n0.223580\n0.653040\n0.612722\n0.004651\n0.162206\n0.069552\n128.615860\n0.464344\n0.748874\n\n\nstd\n0.253540\n0.131416\n0.146133\n0.020891\n0.112612\n0.050678\n27.203857\n0.261281\n0.174302\n\n\nmin\n0.000938\n0.411000\n0.091100\n0.000000\n0.039800\n0.028200\n77.002000\n0.056900\n0.000000\n\n\n25%\n0.033000\n0.553750\n0.546500\n0.000000\n0.098025\n0.041250\n111.751000\n0.243000\n0.694435\n\n\n50%\n0.127000\n0.645500\n0.619500\n0.000002\n0.116500\n0.055000\n122.527500\n0.423500\n0.774785\n\n\n75%\n0.352000\n0.754500\n0.698000\n0.000190\n0.171000\n0.075025\n148.037250\n0.670500\n0.858426\n\n\nmax\n0.959000\n0.943000\n0.946000\n0.135000\n0.556000\n0.303000\n181.489000\n0.934000\n1.000000\n\n\n\n\n\n\n\n\nExtracting the song labels from the dataset is the first crucial step. Then, we must select the appropriate features that will serve as inputs for the Affinity Propagation clustering algorithm from the scikit-learn library. During the clustering process, a preference value of -200 is used to ensure optimal performance. Once the data is inputted, the algorithm is trained to achieve the desired outcome.\n\nlabels = songs.values[:,0]\nX = songs.values[:,1:10]\nkmeans = cluster.AffinityPropagation(preference=-200)\nkmeans.fit(X)\n\nAffinityPropagation(preference=-200)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AffinityPropagationAffinityPropagation(preference=-200)\n\n\nThe script below utilizes a dictionary called “predictions” to keep track of the outcomes of a comparison process between two lists: “kmeans.predict(X)” and “labels”. For each new value, a unique key is generated in the dictionary with the corresponding value from the “labels” list appended to the key’s list of values.\nAfter sorting all values into their designated keys, we proceed to display each key alongside its relevant values. The output displays each category and the corresponding songs it contains.\n\npredictions = {}\nfor p,n in zip(kmeans.predict(X),labels):\n    if not predictions.get(p):\n        predictions[p] = []\n        \n    predictions[p] += [n]\n\nfor p in predictions:\n    print(\"Category\",p)\n    print(\"-----\")\n    for n in predictions[p]:\n        print(n)\n    print(\"\")\n\nCategory 7\n-----\nBeautiful Things\ngreedy\nTEXAS HOLD 'EM\nLovin On Me\nGata Only\nWhatever\nStanding Next to You\n\nCategory 1\n-----\nOKLOSER\ni like the way you kiss me\nScared To Start\nI LUV IT (feat. Playboi Carti)\nFE!N (feat. Playboi Carti)\n\nCategory 2\n-----\nToo Sweet\nwe can't be friends (wait for your love)\nStick Season\nWater\nTraining Season\nFeather\nobsessed\nAustin\nMy Love Mine All Mine\nyes, and?\nHoudini\nNever Lose Me\nMade For Me\nAgora Hills\nMake You Mine\nCONTIGO (with Tiësto)\n\nCategory 0\n-----\nEnd of Beginning\nLose Control\nLike That\n\nCategory 6\n-----\nSaturn\nredrum\nCruel Summer\nSlow It Down\nStrangers\nEyes Closed\n\nCategory 3\n-----\nOne Of The Girls (with JENNIE, Lily Rose Depp)\nWhatever She Wants\nBelong Together\n\nCategory 5\n-----\nII MOST WANTED\nSnooze\nexes\nif u think i'm pretty\nvampire\nRich Baby Daddy (feat. Sexyy Red & SZA)\nType Shit\n\nCategory 4\n-----\nHome\nI Remember Everything (feat. Kacey Musgraves)\nWhat Was I Made For? [From The Motion Picture \"Barbie\"]\n\n\n\nThe script successfully categorized the playlist into 6 distinct groups based on shared features, resulting in a diverse selection of songs within each category.\n\n\nK Means Clustering\nUsing K Means clustering, we choose to break the playlist into 3 smaller playlists.\nAs shown below, we employ the KMeans algorithm, obtained from the sklearn.cluster library, to cluster a collection of songs into distinct categories based on track features, such as their energy levels and sound qualities. Using three clusters, we apply this algorithm on the track features from the “playlist_tracks” subset of data, dropping the “artist” and “name” columns.\n\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\n%matplotlib inline\nplaylist_tracks = df[['artist','name','acousticness','danceability','energy',\n                      'liveness', 'instrumentalness','speechiness','valence']]\n\nkmeans = KMeans(n_clusters = 3)\nkmeans.fit(playlist_tracks.drop(['artist', 'name'], axis = 1))\n\nKMeans(n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3)\n\n\nAdditionally, we import the “seaborn” library and use the “%matplotlib inline” command to generate useful plots displayed inline. Below, we generate a count plot with x-axis values set to a list of groups, displayed as string representations, produced by the k-means clustering algorithm.\n\nsns.countplot(x=[str(group) for group in kmeans.labels_], color = 'lightblue')\n\n\n\n\n\n\n\n\n\n\nVisualizing the Clusters\nMoving forward, let’s look at differences in the audio features of each group.\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(playlist_tracks.drop(['artist', 'name'], axis = 1))\nscaled_data = scaler.transform(playlist_tracks.drop(['artist', 'name'], axis = 1))\n\n\n\nCode\nfrom sklearn.decomposition import PCA\npca = PCA(n_components =2)\npca.fit(scaled_data)\ndata_pca = pca.transform(scaled_data)\n\nplt.scatter(data_pca[:,0], data_pca[:,1], c = list(kmeans.labels_), cmap = 'Paired')\nplt.xlabel('PC1: {:.3f}'.format(pca.explained_variance_ratio_[0]), size = 15)\nplt.ylabel('PC2: {:.3f}'.format(pca.explained_variance_ratio_[1]), size = 15)\n\n\nText(0, 0.5, 'PC2: 0.185')\n\n\n\n\n\n\n\n\n\n\n\nText(0, 0.5, 'PC2: 0.185')\n\n\n\n\n\n\n\n\n\n\nplaylist_tracks['group'] = list(kmeans.labels_)\nplaylist_tracks = playlist_tracks.astype({'group': str})\n\nmeans = pd.DataFrame(index = range(0,3), \n                    columns = list(playlist_tracks[playlist_tracks['group'] == '0'].describe().loc['mean'].index))\nmeans.iloc[0] = playlist_tracks[playlist_tracks['group'] == '0'].describe().loc['mean']\nmeans.iloc[1] = playlist_tracks[playlist_tracks['group'] == '1'].describe().loc['mean']\nmeans.iloc[2] = playlist_tracks[playlist_tracks['group'] == '2'].describe().loc['mean']\nmeans\n\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nenergy\nliveness\ninstrumentalness\nspeechiness\nvalence\n\n\n\n\n0\n0.117477\n0.71855\n0.67045\n0.130555\n0.000861\n0.07814\n0.71945\n\n\n1\n0.634273\n0.550091\n0.485736\n0.206273\n0.01321\n0.047455\n0.370818\n\n\n2\n0.097499\n0.643684\n0.625474\n0.170011\n0.003685\n0.073305\n0.249958",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/01_data.html",
    "href": "01_sentiment_analysis/01_data.html",
    "title": "Extracting Spotify Data",
    "section": "",
    "text": "Getting started, we want to extract data for a set of tracks within one of Spotify’s top-featured playlists. Leveraging the Spotify Web API, we can seamlessly obtain detailed data for a song, such as the performing artist, the album it belongs to, its release date, popularity, and audio features like danceability, energy, and tempo.\nPython libraries like spotipy offer a user-friendly way to interact with the Spotify API, offering a range of functions that streamline tasks like API authentication, retrieving playlist data, and obtaining information about any given song.\n\nAccessing the Spotify Web API\nTo access data from Spotify, we import the spotipy library and the SpotifyClientCredentials module. Additionally, we utilize the pandas package for data manipulation and display. In order to authenticate our access to the Spotify API, we must provide our client ID and client secret to a client credentials manager. Once authenticated, we can use the spotipy module to interact with the Spotify API and retrieve data.\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport pandas as pd\n\nclient_id = \"xxx\"\nclient_secret = \"xxx\"\nmy_auth = SpotifyClientCredentials(client_id, client_secret)\nsp = spotipy.Spotify(auth_manager=my_auth)\n\n\nSpotify’s Featured Playlists\nLet’s take a look at the popular Spotify playlists. Below, the code retrieves a range of Spotify playlists and generates a dataframe containing details for each playlist, including its name, ID, description, thumbnail, total number of tracks, and follower count. The resulting dataframe is displayed as an HTML table.\n\nusername = \"spotify\"\nspotify_playlists = sp.user_playlists(username)\ntop_playlists = get_top_playlists(username, 6)\n\nThe function get_top_playlists retrieves all playlists for a given user and returns the playlists with the most followers. Specifically, the function gets all playlists from a given user, iterating over each playlist item to extract the thumbnail image URL, the playlist name, ID, description, total number of tracks, and follower count for the playlist. It then creates a DataFrame from the playlist data, sorts the DataFrame by the number of followers in descending order, and returns the top playlists with the most followers.\n\n\n\n\n\n\n\nthumbnail\nname\nid\ndescription\ntracks\nfollowers\n\n\n\n\n0\n\nToday’s Top Hits\n37i9dQZF1DXcBWIGoYBM5M\nDoja Cat is on top of the Hottest 50!\n50\n34782068\n\n\n1\n\nRapCaviar\n37i9dQZF1DX0XUsuxWHRQd\nNew music from J. Cole, Lil Wayne and Doja Cat.\n50\n15925885\n\n\n3\n\nViva Latino\n37i9dQZF1DX10zKzsJ2jva\nToday's top Latin hits, elevando nuestra música. Cover: Young Miko\n50\n14927961\n\n\n7\n\nRock Classics\n37i9dQZF1DWXRqgorJj26U\nRock legends & epic songs that continue to inspire generations. Cover: Nirvana\n200\n12122088\n\n\n12\n\nAll Out 2000s\n37i9dQZF1DX4o1oenSJRJd\nThe biggest songs of the 2000s. Cover: Kelly Clarkson\n150\n11163325\n\n\n14\n\nAll Out 80s\n37i9dQZF1DX4UtSsGT1Sbe\nThe biggest songs of the 1980s. Cover: Bonnie Tyler.\n150\n10852998\n\n\n\n\n\n\n\n\n\n\nExtracting Tracks From a Playlist\nThe following script enables the compilation of song and artist data from any Spotify playlist through its URI. To analyze a particular playlist, simply copy the URI from the Spotify Player interface and input it into the function defined below. The get_playlist_tracks method returns a complete list of track IDs and corresponding artists from the selected playlist.\n\ndef get_playlist_tracks(playlist_URI):\n    tracks = []\n    results = sp.playlist_tracks(playlist_URI)\n    tracks = results[\"items\"]\n    while results[\"next\"]:\n        results = sp.next(results)\n        tracks.extend(results[\"items\"])\n    return tracks\n\n\nExtracting Features from Tracks\nThe following script utilizes Spotify’s API to extract further details about each song within the playlist. It obtains metadata such as the track name, the artist it’s sung by, the album it belongs to, the release date, and track features such as danceability, tempo, and popularity.\n\ndef playlist_features(id, artist_id, playlist_id):\n    meta = sp.track(id)\n    audio_features = sp.audio_features(id)\n    artist_info = sp.artist(artist_id)\n    playlist_info = sp.playlist(playlist_id)\n\n    # print(audio_features)\n\n    if audio_features[0] is None:\n        return None\n    \n    \n\n    name = meta['name']\n    track_id = meta['id']\n    album = meta['album']['name']\n    artist = meta['album']['artists'][0]['name']\n    artist_id = meta['album']['artists'][0]['id']\n    release_date = meta['album']['release_date']\n    length = meta['duration_ms']\n    popularity = meta['popularity']\n\n    artist_pop = artist_info[\"popularity\"]\n    artist_genres = artist_info[\"genres\"]\n    artist_followers = artist_info[\"followers\"]['total']\n\n    acousticness = audio_features[0]['acousticness']\n    danceability = audio_features[0]['danceability']\n    energy = audio_features[0]['energy']\n    instrumentalness = audio_features[0]['instrumentalness']\n    liveness = audio_features[0]['liveness']\n    loudness = audio_features[0]['loudness']\n    speechiness = audio_features[0]['speechiness']\n    tempo = audio_features[0]['tempo']\n    valence = audio_features[0]['valence']\n    key = audio_features[0]['key']\n    mode = audio_features[0]['mode']\n    time_signature = audio_features[0]['time_signature']\n    \n    playlist_name = playlist_info['name']\n\n    return [name, track_id, album, artist, artist_id, release_date, length, popularity, \n            artist_pop, artist_genres, artist_followers, acousticness, danceability, \n            energy, instrumentalness, liveness, loudness, speechiness, \n            tempo, valence, key, mode, time_signature, playlist_name]\n\nChoose a specific playlist to analyze by copying the URL from the Spotify Player interface. Using that link, the playlist_tracks method retrieves a list of IDs and corresponding artists for each track from the playlist. Specifically, we analyze Spotify’s Today’s Top Hits playlist.\n\nplaylist_links = [top_playlists['id'][0]]\n\nfor playlist_URI in playlist_links:\n    # playlist_URI = link.split(\"/\")[-1].split(\"?\")[0]\n    \n    all_tracks = [  # Loop over track ids\n    playlist_features(i[\"track\"][\"id\"], i[\"track\"][\"artists\"][0][\"uri\"], playlist_URI)\n    for i in get_playlist_tracks(playlist_URI)\n]\n\nPutting it all together, the get_playlist_tracks function retrieves basic details for each song in a specified Spotify playlist using its URI. The playlist_features function then iterates through these tracks using their IDs to extract additional information, such as danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo, and more. From there, we create a Pandas dataframe by passing in the extracted information.\n\n# Create dataframe\ndf = pd.DataFrame(\n    all_tracks, columns=['name', 'track_id', 'album', 'artist', 'artist_id','release_date',\n                     'length', 'popularity', 'artist_pop', 'artist_genres', 'artist_followers',\n                     'acousticness', 'danceability', 'energy',\n                     'instrumentalness', 'liveness', 'loudness',\n                     'speechiness', 'tempo', 'valence', 'key', 'mode',\n                     'time_signature', 'playlist'])\n\n\nLoudness Scaled\n\n# Loudness Scaled\nfrom sklearn import preprocessing \n\nscaler = preprocessing.MinMaxScaler()\n# scale loudness to fit the same range [0, 1]\nloudness2 = df[\"loudness\"].values\nloudness_scaled=scaler.fit_transform(loudness2.reshape(-1, 1))\ndf['loudness_scaled'] = loudness_scaled",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Extracting Spotify Data</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/02_lyrics.html",
    "href": "01_sentiment_analysis/02_lyrics.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Scraping the Web\nTo get started, the script below imports lyricsgenius, a fundamental package libary allowing for web scraping of the Genius Lyrics website to retrieve the lyrics of any given song. Through the initialization of the genius variable, one can access the Genius API and retrieve the lyrics of any given song, such as “Too Many Nights” by Metro Boomin.\nimport lyricsgenius\n\ngenius = lyricsgenius.Genius(\"epFCxujgBe-Y6WrkZedI8kerKxiCpR6Rh0DAHYNlKDf9B4H1nXTdZIkj7krNUHVV\")\nsong = genius.search_song(\"Too Many Nights\", \"Metro Boomin\")\n\nSearching for \"Too Many Nights\" by Metro Boomin...\nDone.\nFirst, we define a function that retrieves the lyrics for any song and artist from the Genius database. As shown below, it first searches for the track using the provided name and artist and then extracts the lyrics from the search results.\ndef get_song_lyrics(song_name, song_artist):\n    song_genius = genius.search_song(song_name, song_artist)\n    song_lyrics = song_genius.lyrics.partition(\"Lyrics\")[2]\n    # Remove any numbers followed by 'Embed'\n    song_lyrics = re.sub(r\"[\\[].*?[\\]]|\\d+Embed\", \"\", song_lyrics)\n    # Remove text between square brackets\n    song_lyrics = re.sub(r\"(\\-[A-Za-z]+\\-)\", \"\", song_lyrics)\n\n    return song_lyrics",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/02_lyrics.html#sentiment-analysis",
    "href": "01_sentiment_analysis/02_lyrics.html#sentiment-analysis",
    "title": "Natural Language Processing",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nSubsequently, the process involves the implementation of pipeline classes to carry out predictions using models accessible in the Hub. The code imports and employs multiple transformer models specifically designed for text classification and sentiment analysis. Specifically, the following procedure creates three distinct pipelines, each equipped with different models that facilitate the assessment of emotions and sentiment in textual content.\n\nimport transformers\nfrom transformers import pipeline\n\n# Initialize Genius API and sentiment classifiers\nclassifiers = [\n    pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True),\n    pipeline(\"text-classification\", model='cardiffnlp/twitter-roberta-base-sentiment', return_all_scores=True),\n    pipeline(\"sentiment-analysis\", return_all_scores=True)\n]\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\nThe get_lyric_sentiment function takes in pre-processed lyrics as input and produces a dictionary of sentiment scores. It leverages three distinct classifiers to calculate the scores and aggregates them into a final result. For instance, one of these classifiers is the distilbert-base-uncased-emotion model, specifically trained to detect “emotions in texts such as sadness, joy, love, anger, fear, and surprise”.\n\n# Function to perform sentiment analysis\ndef get_lyric_sentiment(lyrics, classifiers):\n    text = ' '.join(lyrics)\n    scores = {}\n    for classifier in classifiers:\n        try:\n            predictions = classifier(text, truncation=True)\n            for prediction in predictions[0]:\n                scores[prediction['label']] = prediction['score']\n        except Exception as e:\n            print(f\"Error during sentiment analysis: {e}\")\n    return scores\n\nIf the lyric sequence contains more than 512 tokens, it will trigger an error message indicating an exception encountered in the ‘embeddings’ layer. However, we have implemented measures to properly manage lyric sequences that exceed 512 words in the function mentioned above.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/02_lyrics.html#putting-it-all-together",
    "href": "01_sentiment_analysis/02_lyrics.html#putting-it-all-together",
    "title": "Natural Language Processing",
    "section": "Putting it All Together",
    "text": "Putting it All Together\nTo summarize, the code efficiently collects data and performs text analysis on every song in a playlist. Specifically, it systematically processes a list of tracks and corresponding artists while simultaneously conducting a thorough cleaning procedure on the lyrics. The cleaning process involves removing all nonessential characters, resulting in a more precise depiction of the song’s content. The outcome is a comprehensive frequency analysis of each word in a song’s lyrics, providing deeper insights into the overall conveyed message.\nAdditionally, the program computes a sentiment score for each song based on the lyrics, indicating whether the lyrics are positive, negative, or neutral. It also collects information about the song and artist, such as the release date, length, popularity, and genre. Finally, the program compiles all this information into a dataframe for further analysis.\n\ntrack_data = []\nfor i, track in all_tracks.iterrows():\n\n    song_name = track[\"name\"] #.partition(\" (\")[0]\n    song_name = track['name'].partition(\" (with\")[0]\n    song_name = song_name.partition(\" - From\")[0]\n\n    artist_name = track[\"artist\"]\n\n    try:\n        track_lyrics = clean_song_lyrics(song_name, artist_name)\n        stopwords_removed = remove_stopwords_lyrics(track_lyrics)\n        lemmatized = word_lemmatize(stopwords_removed)\n\n        sentiment_scores = get_lyric_sentiment(stopwords_removed, classifiers)\n\n        track_info = track.to_dict()\n        track_info.update(sentiment_scores)\n\n        track_info[\"lyrics\"] = track_lyrics\n        track_info[\"stopwords_removed\"] = stopwords_removed\n        track_info[\"lemmatized\"] = lemmatized\n\n        track_data.append(track_info)\n\n    except Exception as e:\n        print(f\"Error processing track {track['name']} by {track['artist']}: {e}\")\n\ndf_tracks = pd.DataFrame(track_data)\n\n\n#df_tracks = pd.DataFrame(track_data)\ndf_tracks.to_csv(\"../assets/data/all_tracks+lyrics.csv\", index=False)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/03_eda.html",
    "href": "01_sentiment_analysis/03_eda.html",
    "title": "EDA",
    "section": "",
    "text": "Visualizing the Data\nOur next step involves visually representing the distribution of different track features, such as song popularity. To achieve this, we use the powerful matplotlib and seaborn libraries.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA</span>"
    ]
  },
  {
    "objectID": "01_sentiment_analysis/03_eda.html#visualizing-the-data",
    "href": "01_sentiment_analysis/03_eda.html#visualizing-the-data",
    "title": "EDA",
    "section": "",
    "text": "Top Artist Genres\nThe following code generates a dictionary that contains the frequency of artist genres, which is then converted into a dataframe sorted by tallying the number of occurrences of each genre The resulting dataframe exhibits the genres and the number of times they occur.\n\ngenres_dict = {}\nfor x in all_tracks.artist_genres:\n    # Converting string to list\n    res = x.replace(\"'\", \"\").strip('][').split(', ')\n    for genre_i in res:\n        if genre_i == \"\":\n            break\n        if genre_i in genres_dict:\n            genres_dict[genre_i] += 1\n        else:\n            genres_dict[genre_i] = 1\n\n# Convert Dictionary to Dataframe\ndf_genres = pd.DataFrame(genres_dict.items(), columns=['Genre', 'Freq']).sort_values('Freq', ascending=False)\ndf_genres = df_genres.reset_index(drop = True)\n\n\n\n\n\n\n\n\n\n\n\n\nArtist Features\nNext, we visualize data for each artist from the playlist track data.\n\n# Count distinct values in column\ntallyArtists = all_tracks.value_counts([\"artist\", \"artist_id\"]).reset_index(name='counts')\ntopArtist = tallyArtists['artist_id'][1]\n\ntallyArtistPop = all_tracks.value_counts([\"artist_pop\"]).reset_index(name='counts')\ntallyPop = all_tracks.value_counts([\"popularity\"]).reset_index(name='counts')\ntallyPop = tallyPop[tallyPop['popularity'] &gt; 0]\n\n\n\n\n        \n        \n\n\n\n\n\nTrack Features\n\n\n                                                \n\n\n\nfig, axs = plt.subplots(3, 3, figsize=(15, 8))\nfig.suptitle(\"Figure 2. Track Feature Distributions\", weight=\"heavy\", y=0.99, x=0.2).set_fontsize(\"18\")\n\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"acousticness\"].notnull()][\"acousticness\"], color=\"#F3E176\", alpha=1.0, ax=axs[0,0], edgecolor=\"#E1C214\", linewidth=1.5, shrink=.8)\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"danceability\"].notnull()][\"danceability\"], color=\"#F9C762\", alpha=1.0, ax=axs[0,1], edgecolor=\"#ECA009\", linewidth=1.5, shrink=.8)\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"energy\"].notnull()][\"energy\"], color=\"#FCB484\", alpha=1.0, ax=axs[0,2], edgecolor=\"#EF6306\", linewidth=1.5, shrink=.8)\n\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"liveness\"].notnull()][\"liveness\"], color=\"#F3E176\", alpha=1.0, ax=axs[1,0], edgecolor=\"#E1C214\", linewidth=1.5, shrink=.8)\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"loudness_scaled\"].notnull()][\"loudness_scaled\"], color=\"#F9C762\", alpha=1.0, ax=axs[1,1], edgecolor=\"#ECA009\", linewidth=1.5, shrink=.8)\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"speechiness\"].notnull()][\"speechiness\"], color=\"#FCB484\", alpha=1.0, ax=axs[1,2], edgecolor=\"#EF6306\", linewidth=1.5, shrink=.8)\n\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"tempo\"].notnull()][\"tempo\"], color=\"#F3E176\", alpha=1.0, ax=axs[2,0], edgecolor=\"#E1C214\", linewidth=1.5, shrink=.8)\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"valence\"].notnull()][\"valence\"], color=\"#F9C762\", alpha=1.0, ax=axs[2,1], edgecolor=\"#ECA009\", linewidth=1.5, shrink=.8)\nsns.histplot(data=all_tracks, x=all_tracks[all_tracks[\"key\"].notnull()][\"key\"], color=\"#FCB484\", alpha=1.0, ax=axs[2,2], edgecolor=\"#EF6306\", linewidth=1.5, shrink=.8)\n\n\n#axs[2].set_title(\"Positive\")\n\nsns.despine(left=True, bottom=True)\nplt.tight_layout()\nplt.savefig(\"../assets/images/features_dist.png\", format=\"png\", dpi=1200)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter-roBERTa-base for Sentiment Analysis\nNow, we present a graphical representation of the results obtained from the roBERTa-base model “trained on roughly 58 million tweets and fine-tuned for sentiment analysis using the TweetEval benchmark” (EMNLP 2020). According to the TweetEval reference paper and official Github repository, the resulting labels 0, 1, and 2 correspond to Negative, Neutral, and Positive, respectively.\n\nLabels: 0 -&gt; Negative; 1 -&gt; Neutral; 2 -&gt; Positive\n\n\n\n\n\n\n\n\n\n\nThis code above creates a histogram that displays the distribution of labels for a roBERTa-base model. The labels are categorized as negative, neutral, or positive, with each having its own distinct color. Additionally, the code adds a title to the figure and resizes the subplots to ensure a better fit. The final output is a graph that can be saved in png format for future reference.\n\n\nCorrelations Matrix\nAfter completing the initial data analysis, we proceed with generating the Pearson correlations matrix using the Pandas command df.corr(). Subsequently, we visualize the matrix using the seaborn heatmap, providing a detailed understanding of the relationships between the various variables in our dataset.\n\ntrack_sentiment_df = df_final[['name', 'artist',\n           'acousticness', 'danceability', 'energy', 'instrumentalness', \n           'loudness', 'speechiness', 'tempo', 'valence', \n           'sadness', 'joy', 'love', 'anger', 'fear', 'surprise',\n           'LABEL_0', 'LABEL_1', 'LABEL_2', 'NEGATIVE', 'POSITIVE']]\n\n# Find the pearson correlations matrix\ncorr = track_sentiment_df.corr(method = 'pearson')\n\n\n\n\n\n\n\n\n\n\nThe code below produces a scatterplot that showcases the correlation between energy and fear. The x-axis represents the energy value, while the y-axis represents the fear sentiment. The size of each data point corresponds to the label indicating the neutral sentiment level, and its color represents the valence value. Moreover, each bubble contains its energy value within, allowing for a straightforward interpretation of the data.\n\n\n\n\n\n\n\n\n\nSimilarly, the scatterplot presented above utilizes the track sentiment data, comparing the energy and fear levels of the tracks based on valence and size.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>EDA</span>"
    ]
  }
]